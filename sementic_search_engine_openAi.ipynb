{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAi sementic search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign openAi api key\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can generate our own document when desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## documents and documents loader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"DOgs are great companions, known for their loyality and friendliness\",\n",
    "        metadata={\"source\": \"mamal-post-doc\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independetn pets that often enjoy their space\",\n",
    "        metadata={\"source\": \"mamal-post-doc\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mamal-post-doc'}, page_content='DOgs are great companions, known for their loyality and friendliness'),\n",
       " Document(metadata={'source': 'mamal-post-doc'}, page_content='Cats are independetn pets that often enjoy their space')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading documents\n",
    "- PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'deep_learning_book.pdf', 'page': 0}, page_content='Deep Learning\\nIan Goodfellow\\nYoshua Bengio\\nAaron Courville'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 1}, page_content='Contents\\nWebsite viii\\nAcknowledgments ix\\nNotation xii\\n1 Introduction 1\\n1.1 Who Should Read This Book? . . . . . . . . . . . . . . . . . . . . 8\\n1.2 Historical Trends in Deep Learning . . . . . . . . . . . . . . . . . 12\\nI Applied Math and Machine Learning Basics 27\\n2 Linear Algebra 29\\n2.1 Scalars, Vectors, Matrices and Tensors . . . . . . . . . . . . . . . 29\\n2.2 Multiplying Matrices and Vectors . . . . . . . . . . . . . . . . . . 32\\n2.3 Identity and Inverse Matrices . . . . . . . . . . . . . . . . . . . . 34\\n2.4 Linear Dependence and Span . . . . . . . . . . . . . . . . . . . . 35\\n2.5 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.6 Special Kinds of Matrices and Vectors . . . . . . . . . . . . . . . 38\\n2.7 Eigendecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n2.8 Singular Value Decomposition . . . . . . . . . . . . . . . . . . . . 42\\n2.9 The Moore-Penrose Pseudoinverse . . . . . . . . . . . . . . . . . . 43\\n2.10 The Trace Operator . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n2.11 The Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n2.12 Example: Principal Components Analysis . . . . . . . . . . . . . 45\\n3 Probability and Information Theory 51\\n3.1 Why Probability? . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\ni'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 2}, page_content='CONTENTS\\n3.2 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n3.3 Probability Distributions . . . . . . . . . . . . . . . . . . . . . . . 54\\n3.4 Marginal Probability . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n3.5 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . 57\\n3.6 The Chain Rule of Conditional Probabilities . . . . . . . . . . . . 57\\n3.7 Independence and Conditional Independence . . . . . . . . . . . . 58\\n3.8 Expectation, Variance and Covariance . . . . . . . . . . . . . . . 58\\n3.9 Common Probability Distributions . . . . . . . . . . . . . . . . . 60\\n3.10 Useful Properties of Common Functions . . . . . . . . . . . . . . 65\\n3.11 Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n3.12 Technical Details of Continuous Variables . . . . . . . . . . . . . 69\\n3.13 Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\n3.14 Structured Probabilistic Models . . . . . . . . . . . . . . . . . . . 73\\n4 Numerical Computation 78\\n4.1 Overﬂow and Underﬂow . . . . . . . . . . . . . . . . . . . . . . . 78\\n4.2 Poor Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n4.3 Gradient-Based Optimization . . . . . . . . . . . . . . . . . . . . 80\\n4.4 Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . 91\\n4.5 Example: Linear Least Squares . . . . . . . . . . . . . . . . . . . 94\\n5 Machine Learning Basics 96\\n5.1 Learning Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 97\\n5.2 Capacity, Overﬁtting and Underﬁtting . . . . . . . . . . . . . . . 108\\n5.3 Hyperparameters and Validation Sets . . . . . . . . . . . . . . . . 118\\n5.4 Estimators, Bias and Variance . . . . . . . . . . . . . . . . . . . . 120\\n5.5 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . 129\\n5.6 Bayesian Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . 133\\n5.7 Supervised Learning Algorithms . . . . . . . . . . . . . . . . . . . 137\\n5.8 Unsupervised Learning Algorithms . . . . . . . . . . . . . . . . . 142\\n5.9 Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . 149\\n5.10 Building a Machine Learning Algorithm . . . . . . . . . . . . . . 151\\n5.11 Challenges Motivating Deep Learning . . . . . . . . . . . . . . . . 152\\nII Deep Networks: Modern Practices 162\\n6 Deep Feedforward Networks 164\\n6.1 Example: Learning XOR . . . . . . . . . . . . . . . . . . . . . . . 167\\n6.2 Gradient-Based Learning . . . . . . . . . . . . . . . . . . . . . . . 172\\nii'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 3}, page_content='CONTENTS\\n6.3 Hidden Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\\n6.4 Architecture Design . . . . . . . . . . . . . . . . . . . . . . . . . . 193\\n6.5 Back-Propagation and Other Diﬀerentiation\\nAlgorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200\\n6.6 Historical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\\n7 Regularization for Deep Learning 224\\n7.1 Parameter Norm Penalties . . . . . . . . . . . . . . . . . . . . . . 226\\n7.2 Norm Penalties as Constrained Optimization . . . . . . . . . . . . 233\\n7.3 Regularization and Under-Constrained Problems . . . . . . . . . 235\\n7.4 Dataset Augmentation . . . . . . . . . . . . . . . . . . . . . . . . 236\\n7.5 Noise Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\\n7.6 Semi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . 240\\n7.7 Multitask Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 241\\n7.8 Early Stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\\n7.9 Parameter Tying and Parameter Sharing . . . . . . . . . . . . . . 249\\n7.10 Sparse Representations . . . . . . . . . . . . . . . . . . . . . . . . 251\\n7.11 Bagging and Other Ensemble Methods . . . . . . . . . . . . . . . 253\\n7.12 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\\n7.13 Adversarial Training . . . . . . . . . . . . . . . . . . . . . . . . . 265\\n7.14 Tangent Distance, Tangent Prop and Manifold\\nTangent Classiﬁer . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\\n8 Optimization for Training Deep Models 271\\n8.1 How Learning Diﬀers from Pure Optimization . . . . . . . . . . . 272\\n8.2 Challenges in Neural Network Optimization . . . . . . . . . . . . 279\\n8.3 Basic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\\n8.4 Parameter Initialization Strategies . . . . . . . . . . . . . . . . . 296\\n8.5 Algorithms with Adaptive Learning Rates . . . . . . . . . . . . . 302\\n8.6 Approximate Second-Order Methods . . . . . . . . . . . . . . . . 307\\n8.7 Optimization Strategies and Meta-Algorithms . . . . . . . . . . . 313\\n9 Convolutional Networks 326\\n9.1 The Convolution Operation . . . . . . . . . . . . . . . . . . . . . 327\\n9.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\\n9.3 Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n9.4 Convolution and Pooling as an Inﬁnitely Strong Prior . . . . . . . 339\\n9.5 Variants of the Basic Convolution Function . . . . . . . . . . . . 342\\n9.6 Structured Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . 352\\n9.7 Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\\niii'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 4}, page_content='CONTENTS\\n9.8 Eﬃcient Convolution Algorithms . . . . . . . . . . . . . . . . . . 356\\n9.9 Random or Unsupervised Features . . . . . . . . . . . . . . . . . 356\\n9.10 The Neuroscientiﬁc Basis for Convolutional\\nNetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358\\n9.11 Convolutional Networks and the History of Deep Learning . . . . 365\\n10 Sequence Modeling: Recurrent and Recursive Nets 367\\n10.1 Unfolding Computational Graphs . . . . . . . . . . . . . . . . . . 369\\n10.2 Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . 372\\n10.3 Bidirectional RNNs . . . . . . . . . . . . . . . . . . . . . . . . . . 388\\n10.4 Encoder-Decoder Sequence-to-Sequence\\nArchitectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390\\n10.5 Deep Recurrent Networks . . . . . . . . . . . . . . . . . . . . . . 392\\n10.6 Recursive Neural Networks . . . . . . . . . . . . . . . . . . . . . . 394\\n10.7 The Challenge of Long-Term Dependencies . . . . . . . . . . . . . 396\\n10.8 Echo State Networks . . . . . . . . . . . . . . . . . . . . . . . . . 399\\n10.9 Leaky Units and Other Strategies for Multiple\\nTime Scales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\\n10.10 The Long Short-Term Memory and Other Gated RNNs . . . . . . 404\\n10.11 Optimization for Long-Term Dependencies . . . . . . . . . . . . . 408\\n10.12 Explicit Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . 412\\n11 Practical Methodology 416\\n11.1 Performance Metrics . . . . . . . . . . . . . . . . . . . . . . . . . 417\\n11.2 Default Baseline Models . . . . . . . . . . . . . . . . . . . . . . . 420\\n11.3 Determining Whether to Gather More Data . . . . . . . . . . . . 421\\n11.4 Selecting Hyperparameters . . . . . . . . . . . . . . . . . . . . . . 422\\n11.5 Debugging Strategies . . . . . . . . . . . . . . . . . . . . . . . . . 431\\n11.6 Example: Multi-Digit Number Recognition . . . . . . . . . . . . . 435\\n12 Applications 438\\n12.1 Large-Scale Deep Learning . . . . . . . . . . . . . . . . . . . . . . 438\\n12.2 Computer Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . 447\\n12.3 Speech Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . 453\\n12.4 Natural Language Processing . . . . . . . . . . . . . . . . . . . . 456\\n12.5 Other Applications . . . . . . . . . . . . . . . . . . . . . . . . . . 473\\niv'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 5}, page_content='CONTENTS\\nIII Deep Learning Research 482\\n13 Linear Factor Models 485\\n13.1 Probabilistic PCA and Factor Analysis . . . . . . . . . . . . . . . 486\\n13.2 Independent Component Analysis (ICA) . . . . . . . . . . . . . . 487\\n13.3 Slow Feature Analysis . . . . . . . . . . . . . . . . . . . . . . . . 489\\n13.4 Sparse Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492\\n13.5 Manifold Interpretation of PCA . . . . . . . . . . . . . . . . . . . 496\\n14 Autoencoders 499\\n14.1 Undercomplete Autoencoders . . . . . . . . . . . . . . . . . . . . 500\\n14.2 Regularized Autoencoders . . . . . . . . . . . . . . . . . . . . . . 501\\n14.3 Representational Power, Layer Size and Depth . . . . . . . . . . . 505\\n14.4 Stochastic Encoders and Decoders . . . . . . . . . . . . . . . . . . 506\\n14.5 Denoising Autoencoders . . . . . . . . . . . . . . . . . . . . . . . 507\\n14.6 Learning Manifolds with Autoencoders . . . . . . . . . . . . . . . 513\\n14.7 Contractive Autoencoders . . . . . . . . . . . . . . . . . . . . . . 518\\n14.8 Predictive Sparse Decomposition . . . . . . . . . . . . . . . . . . 521\\n14.9 Applications of Autoencoders . . . . . . . . . . . . . . . . . . . . 522\\n15 Representation Learning 524\\n15.1 Greedy Layer-Wise Unsupervised Pretraining . . . . . . . . . . . 526\\n15.2 Transfer Learning and Domain Adaptation . . . . . . . . . . . . . 534\\n15.3 Semi-Supervised Disentangling of Causal Factors . . . . . . . . . 539\\n15.4 Distributed Representation . . . . . . . . . . . . . . . . . . . . . . 544\\n15.5 Exponential Gains from Depth . . . . . . . . . . . . . . . . . . . 550\\n15.6 Providing Clues to Discover Underlying Causes . . . . . . . . . . 552\\n16 Structured Probabilistic Models for Deep Learning 555\\n16.1 The Challenge of Unstructured Modeling . . . . . . . . . . . . . . 556\\n16.2 Using Graphs to Describe Model Structure . . . . . . . . . . . . . 560\\n16.3 Sampling from Graphical Models . . . . . . . . . . . . . . . . . . 577\\n16.4 Advantages of Structured Modeling . . . . . . . . . . . . . . . . . 579\\n16.5 Learning about Dependencies . . . . . . . . . . . . . . . . . . . . 579\\n16.6 Inference and Approximate Inference . . . . . . . . . . . . . . . . 580\\n16.7 The Deep Learning Approach to Structured\\nProbabilistic Models . . . . . . . . . . . . . . . . . . . . . . . . . 581\\n17 Monte Carlo Methods 587\\n17.1 Sampling and Monte Carlo Methods . . . . . . . . . . . . . . . . 587\\nv'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 6}, page_content='CONTENTS\\n17.2 Importance Sampling . . . . . . . . . . . . . . . . . . . . . . . . . 589\\n17.3 Markov Chain Monte Carlo Methods . . . . . . . . . . . . . . . . 592\\n17.4 Gibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596\\n17.5 The Challenge of Mixing between Separated\\nModes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597\\n18 Confronting the Partition Function 603\\n18.1 The Log-Likelihood Gradient . . . . . . . . . . . . . . . . . . . . 604\\n18.2 Stochastic Maximum Likelihood and Contrastive Divergence . . . 605\\n18.3 Pseudolikelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . 613\\n18.4 Score Matching and Ratio Matching . . . . . . . . . . . . . . . . 615\\n18.5 Denoising Score Matching . . . . . . . . . . . . . . . . . . . . . . 617\\n18.6 Noise-Contrastive Estimation . . . . . . . . . . . . . . . . . . . . 618\\n18.7 Estimating the Partition Function . . . . . . . . . . . . . . . . . . 621\\n19 Approximate Inference 629\\n19.1 Inference as Optimization . . . . . . . . . . . . . . . . . . . . . . 631\\n19.2 Expectation Maximization . . . . . . . . . . . . . . . . . . . . . . 632\\n19.3 MAP Inference and Sparse Coding . . . . . . . . . . . . . . . . . 633\\n19.4 Variational Inference and Learning . . . . . . . . . . . . . . . . . 636\\n19.5 Learned Approximate Inference . . . . . . . . . . . . . . . . . . . 648\\n20 Deep Generative Models 651\\n20.1 Boltzmann Machines . . . . . . . . . . . . . . . . . . . . . . . . . 651\\n20.2 Restricted Boltzmann Machines . . . . . . . . . . . . . . . . . . . 653\\n20.3 Deep Belief Networks . . . . . . . . . . . . . . . . . . . . . . . . . 657\\n20.4 Deep Boltzmann Machines . . . . . . . . . . . . . . . . . . . . . . 660\\n20.5 Boltzmann Machines for Real-Valued Data . . . . . . . . . . . . . 673\\n20.6 Convolutional Boltzmann Machines . . . . . . . . . . . . . . . . . 679\\n20.7 Boltzmann Machines for Structured or Sequential Outputs . . . . 681\\n20.8 Other Boltzmann Machines . . . . . . . . . . . . . . . . . . . . . 683\\n20.9 Back-Propagation through Random Operations . . . . . . . . . . 684\\n20.10 Directed Generative Nets . . . . . . . . . . . . . . . . . . . . . . . 688\\n20.11 Drawing Samples from Autoencoders . . . . . . . . . . . . . . . . 707\\n20.12 Generative Stochastic Networks . . . . . . . . . . . . . . . . . . . 710\\n20.13 Other Generation Schemes . . . . . . . . . . . . . . . . . . . . . . 712\\n20.14 Evaluating Generative Models . . . . . . . . . . . . . . . . . . . . 713\\n20.15 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 716\\nBibliography 717\\nvi'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 7}, page_content='CONTENTS\\nIndex 773\\nvii'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 8}, page_content='Website\\nwww.deeplearningbook.org\\nThis book is accompanied by the above website. The website provides a\\nvariety of supplementary material, including exercises, lecture slides, corrections of\\nmistakes, and other resources that should be useful to both readers and instructors.\\nviii'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 9}, page_content='Bibliography\\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis,\\nA., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M.,\\nJia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mané, D., Monga, R.,\\nMoore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I.,\\nTalwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viégas, F., Vinyals, O., Warden,\\nP., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. (2015). TensorFlow: Large-scale\\nmachine learning on heterogeneous systems. Software available from tensorﬂow.org. 25,\\n210, 441\\nAckley, D. H., Hinton, G. E., and Sejnowski, T. J. (1985). A learning algorithm for\\nBoltzmann machines.Cognitive Science, 9, 147–169. 567, 651\\nAlain, G. and Bengio, Y. (2013). What regularized auto-encoders learn from the data\\ngenerating distribution. InICLR’2013, arXiv:1211.4246. 504, 509, 512, 518\\nAlain, G., Bengio, Y., Yao, L., Éric Thibodeau-Laufer, Yosinski, J., and Vincent, P. (2015).\\nGSNs: Generative stochastic networks. arXiv:1503.05571. 507, 709\\nAllen, R. B. (1987). Several studies on natural language and back-propagation. InIEEE\\nFirst International Conference on Neural Networks, volume 2, pages 335–341, San\\nDiego. 468\\nAnderson, E. (1935). The Irises of the Gaspé Peninsula.Bulletin of the American Iris\\nSociety, 59, 2–5. 19\\nBa, J., Mnih, V., and Kavukcuoglu, K. (2014). Multiple object recognition with visual\\nattention. arXiv:1412.7755. 688\\nBachman, P. and Precup, D. (2015). Variational generative stochastic networks with\\ncollaborative shaping. InProceedings of the 32nd International Conference on Machine\\nLearning, ICML 2015, Lille, France, 6-11 July 2015, pages 1964–1972. 713\\nBacon, P.-L., Bengio, E., Pineau, J., and Precup, D. (2015). Conditional computation in\\nneural networks using a decision-theoretic approach. In2nd Multidisciplinary Conference\\non Reinforcement Learning and Decision Making (RLDM 2015). 445\\n717'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 10}, page_content='BIBLIOGRAPHY\\nBagnell, J. A. and Bradley, D. M. (2009). Diﬀerentiable sparse coding. In D. Koller,\\nD. Schuurmans, Y. Bengio, and L. Bottou, editors,Advances in Neural Information\\nProcessing Systems 21 (NIPS’08), pages 113–120. 494\\nBahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine translation by jointly\\nlearning to align and translate. InICLR’2015, arXiv:1409.0473. 25, 99, 392, 412, 415,\\n459, 470, 471\\nBahl, L. R., Brown, P., de Souza, P. V., and Mercer, R. L. (1987). Speech recognition\\nwith continuous-parameter hidden Markov models.Computer, Speech and Language, 2,\\n219–234. 453\\nBaldi, P. and Hornik, K. (1989). Neural networks and principal component analysis:\\nLearning from examples without local minima.Neural Networks, 2, 53–58. 283\\nBaldi, P., Brunak, S., Frasconi, P., Soda, G., and Pollastri, G. (1999). Exploiting the\\npast and the future in protein secondary structure prediction.Bioinformatics, 15(11),\\n937–946. 388\\nBaldi, P., Sadowski, P., and Whiteson, D. (2014). Searching for exotic particles in\\nhigh-energy physics with deep learning.Nature communications, 5. 26\\nBallard, D. H., Hinton, G. E., and Sejnowski, T. J. (1983). Parallel vision computation.\\nNature. 447\\nBarlow, H. B. (1989). Unsupervised learning.Neural Computation, 1, 295–311. 144\\nBarron, A. E. (1993). Universal approximation bounds for superpositions of a sigmoidal\\nfunction. IEEE Trans. on Information Theory, 39, 930–945. 195\\nBartholomew, D. J. (1987).Latent variable models and factor analysis. Oxford University\\nPress. 486\\nBasilevsky, A. (1994). Statistical Factor Analysis and Related Methods: Theory and\\nApplications. Wiley. 486\\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A.,\\nBouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements.\\nDeep Learning and Unsupervised Feature Learning NIPS 2012 Workshop. 25, 80, 210,\\n218, 441\\nBasu, S. and Christensen, J. (2013). Teaching classiﬁcation boundaries to humans. In\\nAAAI’2013. 325\\nBaxter, J. (1995). Learning internal representations. InProceedings of the 8th International\\nConference on Computational Learning Theory (COLT’95), pages 311–320, Santa Cruz,\\nCalifornia. ACM Press. 241\\n718'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 11}, page_content='BIBLIOGRAPHY\\nBayer, J. and Osendorfer, C. (2014). Learning stochastic recurrent networks.ArXiv\\ne-prints. 262\\nBecker, S. and Hinton, G. (1992). A self-organizing neural network that discovers surfaces\\nin random-dot stereograms.Nature, 355, 161–163. 539\\nBehnke, S. (2001). Learning iterative image reconstruction in the neural abstraction\\npyramid. Int. J. Computational Intelligence and Applications, 1(4), 427–438. 511\\nBeiu, V., Quintana, J. M., and Avedillo, M. J. (2003). VLSI implementations of threshold\\nlogic-a comprehensive survey.Neural Networks, IEEE Transactions on, 14(5), 1217–\\n1243. 446\\nBelkin, M. and Niyogi, P. (2002). Laplacian eigenmaps and spectral techniques for\\nembedding and clustering. In T. Dietterich, S. Becker, and Z. Ghahramani, editors,\\nAdvances in Neural Information Processing Systems 14 (NIPS’01), Cambridge, MA.\\nMIT Press. 240\\nBelkin, M. and Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and\\ndata representation.Neural Computation, 15(6), 1373–1396. 160, 516\\nBengio, E., Bacon, P.-L., Pineau, J., and Precup, D. (2015a). Conditional computation in\\nneural networks for faster models. arXiv:1511.06297. 445\\nBengio, S. and Bengio, Y. (2000a). Taking on the curse of dimensionality in joint\\ndistributions using neural networks.IEEE Transactions on Neural Networks, special\\nissue on Data Mining and Knowledge Discovery, 11(3), 550–557. 703\\nBengio, S., Vinyals, O., Jaitly, N., and Shazeer, N. (2015b). Scheduled sampling for\\nsequence prediction with recurrent neural networks. Technical report, arXiv:1506.03099.\\n378\\nBengio, Y.(1991). Artiﬁcial Neural Networks and their Application to Sequence Recognition.\\nPh.D. thesis, McGill University, (Computer Science), Montreal, Canada. 402\\nBengio, Y. (2000). Gradient-based optimization of hyperparameters.Neural Computation,\\n12(8), 1889–1900. 430\\nBengio, Y. (2002). New distributed probabilistic language models. Technical Report 1215,\\nDept. IRO, Université de Montréal. 462\\nBengio, Y. (2009).Learning deep architectures for AI. Now Publishers. 197, 621\\nBengio, Y. (2013). Deep learning of representations: looking forward. In Statistical\\nLanguage and Speech Processing, volume 7978 ofLecture Notes in Computer Science,\\npages 1–37. Springer, also in arXiv at http://arxiv.org/abs/1305.0445. 443\\nBengio, Y. (2015). Early inference in energy-based models approximates back-propagation.\\nTechnical Report arXiv:1510.02777, Universite de Montreal. 653\\n719'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 12}, page_content='BIBLIOGRAPHY\\nBengio, Y. and Bengio, S. (2000b). Modeling high-dimensional discrete data with multi-\\nlayer neural networks. InNIPS 12, pages 400–406. MIT Press. 702, 703, 705, 707\\nBengio, Y. and Delalleau, O. (2009). Justifying and generalizing contrastive divergence.\\nNeural Computation, 21(6), 1601–1621. 509, 609\\nBengio, Y. and Grandvalet, Y. (2004). No unbiased estimator of the variance of k-fold\\ncross-validation. In S. Thrun, L. Saul, and B. Schölkopf, editors,Advances in Neural\\nInformation Processing Systems 16 (NIPS’03), Cambridge, MA. MIT Press, Cambridge.\\n120\\nBengio, Y. and LeCun, Y. (2007). Scaling learning algorithms towards AI. InLarge Scale\\nKernel Machines. 18\\nBengio, Y. and Monperrus, M. (2005). Non-local manifold tangent learning. In L. Saul,\\nY. Weiss, and L. Bottou, editors,Advances in Neural Information Processing Systems\\n17 (NIPS’04), pages 129–136. MIT Press. 157, 518\\nBengio, Y. and Sénécal, J.-S. (2003). Quick training of probabilistic neural nets by\\nimportance sampling. InProceedings of AISTATS 2003. 465\\nBengio, Y. and Sénécal, J.-S. (2008). Adaptive importance sampling to accelerate training\\nof a neural probabilistic language model.IEEE Trans. Neural Networks, 19(4), 713–722.\\n465\\nBengio, Y., De Mori, R., Flammia, G., and Kompe, R. (1991). Phonetically motivated\\nacoustic parameters for continuous speech recognition using artiﬁcial neural networks.\\nIn Proceedings of EuroSpeech’91. 23, 454\\nBengio, Y., De Mori, R., Flammia, G., and Kompe, R. (1992). Neural network-Gaussian\\nmixture hybrid for speech recognition or density estimation. InNIPS 4, pages 175–182.\\nMorgan Kaufmann. 454\\nBengio, Y., Frasconi, P., and Simard, P. (1993). The problem of learning long-term\\ndependencies in recurrent networks. In IEEE International Conference on Neural\\nNetworks, pages 1183–1195, San Francisco. IEEE Press. (invited paper). 398\\nBengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with\\ngradient descent is diﬃcult.IEEE Tr. Neural Nets. 17, 396, 398, 399, 407\\nBengio, Y., Latendresse, S., and Dugas, C. (1999). Gradient-based learning of hyper-\\nparameters. Learning Conference, Snowbird. 430\\nBengio, Y., Ducharme, R., and Vincent, P. (2001). A neural probabilistic language model.\\nIn T. K. Leen, T. G. Dietterich, and V. Tresp, editors,NIPS’2000, pages 932–938. MIT\\nPress. 17, 442, 458, 461, 467, 472, 477\\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A neural probabilistic\\nlanguage model. JMLR, 3, 1137–1155. 461, 467\\n720'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 13}, page_content='BIBLIOGRAPHY\\nBengio, Y., Le Roux, N., Vincent, P., Delalleau, O., and Marcotte, P. (2006a). Convex\\nneural networks. InNIPS’2005, pages 123–130. 255\\nBengio, Y., Delalleau, O., and Le Roux, N. (2006b). The curse of highly variable functions\\nfor local kernel machines. InNIPS’2005. 155\\nBengio, Y., Larochelle, H., and Vincent, P. (2006c). Non-local manifold Parzen windows.\\nIn NIPS’2005. MIT Press. 157, 517\\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy layer-wise\\ntraining of deep networks. InNIPS’2006. 13, 18, 197, 319, 320, 526, 528\\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In\\nICML’09. 324\\nBengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013a). Better mixing via deep\\nrepresentations. InICML’2013. 601\\nBengio, Y., Léonard, N., and Courville, A. (2013b). Estimating or propagating gradients\\nthrough stochastic neurons for conditional computation. arXiv:1308.3432. 443, 445,\\n685, 688\\nBengio, Y., Yao, L., Alain, G., and Vincent, P. (2013c). Generalized denoising auto-\\nencoders as generative models. InNIPS’2013. 504, 708, 709\\nBengio, Y., Courville, A., and Vincent, P. (2013d). Representation learning: A review and\\nnew perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI),\\n35(8), 1798–1828. 552\\nBengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014). Deep generative\\nstochastic networks trainable by backprop. InICML’2014. 708, 709, 710, 711\\nBennett, C. (1976). Eﬃcient estimation of free energy diﬀerences from Monte Carlo data.\\nJournal of Computational Physics, 22(2), 245–268. 627\\nBennett, J. and Lanning, S. (2007). The Netﬂix prize. 475\\nBerger, A. L., Della Pietra, V. J., and Della Pietra, S. A. (1996). A maximum entropy\\napproach to natural language processing.Computational Linguistics, 22, 39–71. 468\\nBerglund, M. and Raiko, T. (2013). Stochastic gradient estimate variance in contrastive\\ndivergence and persistent contrastive divergence.CoRR, abs/1312.6002. 612\\nBergstra, J. (2011). Incorporating Complex Cells into Neural Networks for Pattern\\nClassiﬁcation. Ph.D. thesis, Université de Montréal. 252\\nBergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for pretraining complex\\ncell-like networks. InNIPS’2009. 490\\n721'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 14}, page_content='BIBLIOGRAPHY\\nBergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization.J.\\nMachine Learning Res., 13, 281–305. 428, 429\\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian,\\nJ., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression\\ncompiler. In Proc. SciPy. 25, 80, 210, 218, 441\\nBergstra, J., Bardenet, R., Bengio, Y., andKégl, B.(2011). Algorithmsforhyper-parameter\\noptimization. In NIPS’2011. 430\\nBerkes, P. and Wiskott, L. (2005). Slow feature analysis yields a rich repertoire of complex\\ncell properties. Journal of Vision, 5(6), 579–602. 491\\nBertsekas, D. P. and Tsitsiklis, J. (1996).Neuro-Dynamic Programming. Athena Scientiﬁc.\\n104\\nBesag, J. (1975). Statistical analysis of non-lattice data.The Statistician, 24(3), 179–195.\\n613\\nBishop, C. M. (1994). Mixture density networks. 185\\nBishop, C. M. (1995a). Regularization and complexity control in feed-forward networks.\\nIn Proceedings International Conference on Artiﬁcial Neural Networks ICANN’95,\\nvolume 1, page 141–148. 238, 247\\nBishop, C. M. (1995b). Training with noise is equivalent to Tikhonov regularization.\\nNeural Computation, 7(1), 108–116. 238\\nBishop, C. M. (2006).Pattern Recognition and Machine Learning. Springer. 96, 142\\nBlum, A. L. and Rivest, R. L. (1992). Training a 3-node neural network is NP-complete.\\n289\\nBlumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. (1989). Learnability and\\nthe Vapnik–Chervonenkis dimension.Journal of the ACM, 36(4), 929––865. 112\\nBonnet, G. (1964). Transformations des signaux aléatoires à travers les systèmes non\\nlinéaires sans mémoire.Annales des Télécommunications, 19(9–10), 203–220. 685\\nBordes, A., Weston, J., Collobert, R., and Bengio, Y. (2011). Learning structured\\nembeddings of knowledge bases. InAAAI 2011. 479\\nBordes, A., Glorot, X., Weston, J., and Bengio, Y. (2012). Joint learning of words and\\nmeaning representations for open-text semantic parsing.AISTATS’2012. 396, 479, 480\\nBordes, A., Glorot, X., Weston, J., and Bengio, Y. (2013a). A semantic matching energy\\nfunction for learning with multi-relational data.Machine Learning: Special Issue on\\nLearning Semantics. 479\\n722'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 15}, page_content='BIBLIOGRAPHY\\nBordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. (2013b).\\nTranslating embeddings for modeling multi-relational data. In C. Burges, L. Bottou,\\nM.Welling, Z.Ghahramani, andK.Weinberger, editors, Advances in Neural Information\\nProcessing Systems 26, pages 2787–2795. Curran Associates, Inc. 479\\nBornschein, J. and Bengio, Y. (2015). Reweighted wake-sleep. In ICLR’2015,\\narXiv:1406.2751. 690\\nBornschein, J., Shabanian, S., Fischer, A., and Bengio, Y. (2015). Training bidirectional\\nHelmholtz machines. Technical report, arXiv:1506.03877. 690\\nBoser, B. E., Guyon, I. M., and Vapnik, V. N. (1992). A training algorithm for opti-\\nmal margin classiﬁers. In COLT ’92: Proceedings of the ﬁfth annual workshop on\\nComputational learning theory, pages 144–152, New York, NY, USA. ACM. 17, 139\\nBottou, L. (1998). Online algorithms and stochastic approximations. In D. Saad, editor,\\nOnline Learning in Neural Networks. Cambridge University Press, Cambridge, UK. 292\\nBottou, L. (2011). From machine learning to machine reasoning. Technical report,\\narXiv.1102.1808. 394, 396\\nBottou, L. (2015). Multilayer neural networks. Deep Learning Summer School. 434\\nBottou, L. and Bousquet, O. (2008). The tradeoﬀs of large scale learning. InNIPS’2008.\\n279, 292\\nBoulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012). Modeling temporal\\ndependencies in high-dimensional sequences: Application to polyphonic music generation\\nand transcription. InICML’12. 682\\nBoureau, Y., Ponce, J., and LeCun, Y. (2010). A theoretical analysis of feature pooling in\\nvision algorithms. InProc. International Conference on Machine learning (ICML’10).\\n339\\nBoureau, Y., Le Roux, N., Bach, F., Ponce, J., and LeCun, Y. (2011). Ask the locals:\\nmulti-way local pooling for image recognition. InProc. International Conference on\\nComputer Vision (ICCV’11). IEEE. 339\\nBourlard, H. and Kamp, Y. (1988). Auto-association by multilayer perceptrons and\\nsingular value decomposition.Biological Cybernetics, 59, 291–294. 499\\nBourlard, H. and Wellekens, C. (1989). Speech pattern discrimination and multi-layered\\nperceptrons. Computer Speech and Language, 3, 1–19. 454\\nBoyd, S. and Vandenberghe, L. (2004).Convex Optimization. Cambridge University\\nPress, New York, NY, USA. 91\\n723'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 16}, page_content='BIBLIOGRAPHY\\nBrady, M. L., Raghavan, R., and Slawny, J. (1989). Back-propagation fails to separate\\nwhere perceptrons succeed.IEEE Transactions on Circuits and Systems, 36, 665–674.\\n282\\nBrakel, P., Stroobandt, D., and Schrauwen, B. (2013). Training energy-based models for\\ntime-series imputation. Journal of Machine Learning Research, 14, 2771–2797. 671,\\n695\\nBrand, M. (2003). Charting a manifold. InNIPS’2002, pages 961–968. MIT Press. 160,\\n516\\nBreiman, L. (1994). Bagging predictors.Machine Learning, 24(2), 123–140. 253\\nBreiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. (1984).Classiﬁcation and\\nRegression Trees. Wadsworth International Group, Belmont, CA. 142\\nBridle, J. S. (1990). Alphanets: a recurrent ‘neural’ network architecture with a hidden\\nMarkov model interpretation.Speech Communication, 9(1), 83–92. 182\\nBriggman, K., Denk, W., Seung, S., Helmstaedter, M. N., and Turaga, S. C. (2009).\\nMaximin aﬃnity learning of image segmentation. InNIPS’2009, pages 1865–1873. 353\\nBrown, P. F., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Laﬀerty, J. D.,\\nMercer, R. L., and Roossin, P. S. (1990). A statistical approach to machine translation.\\nComputational linguistics, 16(2), 79–85. 19\\nBrown, P. F., Pietra, V. J. D., DeSouza, P. V., Lai, J. C., and Mercer, R. L. (1992). Class-\\nbased n-gram models of natural language.Computational Linguistics, 18, 467–479.\\n458\\nBryson, A. and Ho, Y. (1969).Applied optimal control: optimization, estimation, and\\ncontrol. Blaisdell Pub. Co. 221\\nBryson, Jr., A. E. and Denham, W. F. (1961). A steepest-ascent method for solving\\noptimum programming problems. Technical Report BR-1303, Raytheon Company,\\nMissle and Space Division. 221\\nBuciluˇ a, C., Caruana, R., and Niculescu-Mizil, A. (2006). Model compression. In\\nProceedings of the 12th ACM SIGKDD international conference on Knowledge discovery\\nand data mining, pages 535–541. ACM. 443\\nBurda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders.\\narXiv preprint arXiv:1509.00519. 695\\nCai, M., Shi, Y., and Liu, J. (2013). Deep maxout neural networks for speech recognition.\\nIn Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop\\non, pages 291–296. IEEE. 190\\n724'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 17}, page_content='BIBLIOGRAPHY\\nCarreira-Perpiñan, M. A. and Hinton, G. E. (2005). On contrastive divergence learning.\\nIn R. G. Cowell and Z. Ghahramani, editors,Proceedings of the Tenth International\\nWorkshop on Artiﬁcial Intelligence and Statistics (AISTATS’05), pages 33–40. Society\\nfor Artiﬁcial Intelligence and Statistics. 609\\nCaruana, R. (1993). Multitask connectionist learning. InProc. 1993 Connectionist Models\\nSummer School, pages 372–379. 241\\nCauchy, A. (1847). Méthode générale pour la résolution de systèmes d’équations simul-\\ntanées. In Compte rendu des séances de l’académie des sciences, pages 536–538. 81,\\n221\\nCayton, L. (2005). Algorithms for manifold learning. Technical Report CS2008-0923,\\nUCSD. 160\\nChandola, V., Banerjee, A., and Kumar, V. (2009). Anomaly detection: A survey.ACM\\ncomputing surveys (CSUR), 41(3), 15. 100\\nChapelle, O., Weston, J., and Schölkopf, B. (2003). Cluster kernels for semi-supervised\\nlearning. In S. Becker, S. Thrun, and K. Obermayer, editors,Advances in Neural\\nInformation Processing Systems 15 (NIPS’02), pages 585–592, Cambridge, MA. MIT\\nPress. 240\\nChapelle, O., Schölkopf, B., and Zien, A., editors (2006).Semi-Supervised Learning. MIT\\nPress, Cambridge, MA. 240, 539\\nChellapilla, K., Puri, S., and Simard, P. (2006). High Performance Convolutional Neural\\nNetworks for Document Processing. In Guy Lorette, editor, Tenth International\\nWorkshop on Frontiers in Handwriting Recognition, La Baule (France). Université de\\nRennes 1, Suvisoft. http://www.suvisoft.com. 22, 23, 440\\nChen, B., Ting, J.-A., Marlin, B. M., and de Freitas, N. (2010). Deep learning of invariant\\nspatio-temporal features from video. NIPS*2010 Deep Learning and Unsupervised\\nFeature Learning Workshop. 354\\nChen, S. F. and Goodman, J. T. (1999). An empirical study of smoothing techniques for\\nlanguage modeling. Computer, Speech and Language, 13(4), 359–393. 457, 468\\nChen, T., Du, Z., Sun, N., Wang, J., Wu, C., Chen, Y., and Temam, O. (2014a). DianNao:\\nA small-footprint high-throughput accelerator for ubiquitous machine-learning. InPro-\\nceedings of the 19th international conference on Architectural support for programming\\nlanguages and operating systems, pages 269–284. ACM. 446\\nChen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang, C.,\\nand Zhang, Z. (2015). MXNet: A ﬂexible and eﬃcient machine learning library for\\nheterogeneous distributed systems.arXiv preprint arXiv:1512.01274. 25\\n725'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 18}, page_content='BIBLIOGRAPHY\\nChen, Y., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L., Chen, T., Xu, Z., Sun, N.,\\net al.(2014b). DaDianNao: A machine-learning supercomputer. InMicroarchitecture\\n(MICRO), 2014 47th Annual IEEE/ACM International Symposium on, pages 609–622.\\nIEEE. 446\\nChilimbi, T., Suzue, Y., Apacible, J., and Kalyanaraman, K. (2014). Project Adam:\\nBuilding an eﬃcient and scalable deep learning training system. In11th USENIX\\nSymposium on Operating Systems Design and Implementation (OSDI’14). 442\\nCho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is eﬃcient for learning restricted\\nBoltzmann machines. InIJCNN’2010. 601, 612\\nCho, K., Raiko, T., and Ilin, A. (2011). Enhanced gradient and adaptive learning rate for\\ntraining restricted Boltzmann machines. InICML’2011, pages 105–112. 670\\nCho, K., van Merriënboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y.\\n(2014a). Learning phrase representations using RNN encoder-decoder for statistical\\nmachine translation. InProceedings of the Empiricial Methods in Natural Language\\nProcessing (EMNLP 2014). 390, 469, 470\\nCho, K., Van Merriënboer, B., Bahdanau, D., and Bengio, Y. (2014b). On the prop-\\nerties of neural machine translation: Encoder-decoder approaches. ArXiv e-prints,\\nabs/1409.1259. 407\\nChoromanska, A., Henaﬀ, M., Mathieu, M., Arous, G. B., and LeCun, Y. (2014). The\\nloss surface of multilayer networks. 282, 283\\nChorowski, J., Bahdanau, D., Cho, K., and Bengio, Y. (2014). End-to-end continuous\\nspeech recognition using attention-based recurrent NN: First results. arXiv:1412.1602.\\n455\\nChrisman, L. (1991). Learning recursive distributed representations for holistic computa-\\ntion. Connection Science, 3(4), 345–366. 468\\nChristianson, B. (1992). Automatic Hessians by reverse accumulation.IMA Journal of\\nNumerical Analysis, 12(2), 135–150. 220\\nChrupala, G., Kadar, A., and Alishahi, A. (2015). Learning language through pictures.\\narXiv 1506.03694. 407\\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical evaluation of gated\\nrecurrent neural networks on sequence modeling. NIPS’2014 Deep Learning workshop,\\narXiv 1412.3555. 407, 455\\nChung, J., Gülçehre, Ç., Cho, K., and Bengio, Y. (2015a). Gated feedback recurrent\\nneural networks. InICML’15. 407\\nChung, J., Kastner, K., Dinh, L., Goel, K., Courville, A., and Bengio, Y. (2015b). A\\nrecurrent latent variable model for sequential data. InNIPS’2015. 694\\n726'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 19}, page_content='BIBLIOGRAPHY\\nCiresan, D., Meier, U., Masci, J., and Schmidhuber, J. (2012). Multi-column deep neural\\nnetwork for traﬃc sign classiﬁcation.Neural Networks, 32, 333–338. 24, 197\\nCiresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. (2010). Deep big\\nsimple neural nets for handwritten digit recognition.Neural Computation, 22, 1–14.\\n22, 23, 441\\nCoates, A. and Ng, A. Y. (2011). The importance of encoding versus training with sparse\\ncoding and vector quantization. InICML’2011. 23, 252, 494\\nCoates, A., Lee, H., and Ng, A. Y. (2011). An analysis of single-layer networks in\\nunsupervised feature learning. InProceedings of the Thirteenth International Conference\\non Artiﬁcial Intelligence and Statistics (AISTATS 2011). 357, 450\\nCoates, A., Huval, B., Wang, T., Wu, D., Catanzaro, B., and Andrew, N. (2013).\\nDeep learning with COTS HPC systems. In S. Dasgupta and D. McAllester, editors,\\nProceedings of the 30th International Conference on Machine Learning (ICML-13),\\nvolume 28 (3), pages 1337–1345. JMLR Workshop and Conference Proceedings. 22, 23,\\n358, 442\\nCohen, N., Sharir, O., and Shashua, A. (2015). On the expressive power of deep learning:\\nA tensor analysis. arXiv:1509.05009. 552\\nCollobert, R. (2004).Large Scale Machine Learning. Ph.D. thesis, Université de Paris VI,\\nLIP6. 193\\nCollobert, R. (2011). Deep learning for eﬃcient discriminative parsing. InAISTATS’2011.\\n99, 473\\nCollobert, R.andWeston, J.(2008a). Auniﬁedarchitecturefornaturallanguageprocessing:\\nDeep neural networks with multitask learning. InICML’2008. 466, 473\\nCollobert, R. and Weston, J. (2008b). A uniﬁed architecture for natural language\\nprocessing: Deep neural networks with multitask learning. InICML’2008. 533\\nCollobert, R., Bengio, S., and Bengio, Y. (2001). A parallel mixture of SVMs for very\\nlarge scale problems. Technical Report IDIAP-RR-01-12, IDIAP. 445\\nCollobert, R., Bengio, S., and Bengio, Y. (2002). Parallel mixture of SVMs for very large\\nscale problems. Neural Computation, 14(5), 1105–1114. 445\\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011a).\\nNatural language processing (almost) from scratch.The Journal of Machine Learning\\nResearch, 12, 2493–2537. 324, 473, 533, 534\\nCollobert, R., Kavukcuoglu, K., and Farabet, C. (2011b). Torch7: A Matlab-like environ-\\nment for machine learning. InBigLearn, NIPS Workshop. 25, 209, 441\\n727'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 20}, page_content='BIBLIOGRAPHY\\nComon, P. (1994). Independent component analysis - a new concept?Signal Processing,\\n36, 287–314. 487\\nCortes, C. and Vapnik, V. (1995). Support vector networks.Machine Learning, 20,\\n273–297. 17, 139\\nCouprie, C., Farabet, C., Najman, L., and LeCun, Y. (2013). Indoor semantic segmentation\\nusing depth information. In International Conference on Learning Representations\\n(ICLR2013). 24, 197\\nCourbariaux, M., Bengio, Y., and David, J.-P. (2015). Low precision arithmetic for deep\\nlearning. In Arxiv:1412.7024, ICLR’2015 Workshop. 447\\nCourville, A., Bergstra, J., and Bengio, Y. (2011). Unsupervised models of images by\\nspike-and-slab RBMs. InICML’11. 558, 677\\nCourville, A., Desjardins, G., Bergstra, J., and Bengio, Y. (2014). The spike-and-slab\\nRBM and extensions to discrete and sparse data distributions.Pattern Analysis and\\nMachine Intelligence, IEEE Transactions on, 36(9), 1874–1887. 679\\nCover, T. M. and Thomas, J. A. (2006).Elements of Information Theory, 2nd Edition.\\nWiley-Interscience. 71\\nCox, D. and Pinto, N. (2011). Beyond simple features: A large-scale feature search\\napproach to unconstrained face recognition. InAutomatic Face & Gesture Recognition\\nand Workshops (FG 2011), 2011 IEEE International Conference on, pages 8–15. IEEE.\\n357\\nCramér, H. (1946).Mathematical methods of statistics. Princeton University Press. 133,\\n292\\nCrick, F. H. C. and Mitchison, G. (1983). The function of dream sleep.Nature, 304,\\n111–114. 607\\nCybenko, G.(1989). Approximationbysuperpositionsofasigmoidalfunction. Mathematics\\nof Control, Signals, and Systems, 2, 303–314. 194\\nDahl, G. E., Ranzato, M., Mohamed, A., and Hinton, G. E. (2010). Phone recognition\\nwith the mean-covariance restricted Boltzmann machine. InNIPS’2010. 24\\nDahl, G. E., Yu, D., Deng, L., and Acero, A. (2012). Context-dependent pre-trained deep\\nneural networks for large vocabulary speech recognition.IEEE Transactions on Audio,\\nSpeech, and Language Processing, 20(1), 33–42. 454\\nDahl, G. E., Sainath, T. N., and Hinton, G. E. (2013). Improving deep neural networks\\nfor LVCSR using rectiﬁed linear units and dropout. InICASSP’2013. 454\\nDahl, G. E., Jaitly, N., and Salakhutdinov, R. (2014). Multi-task neural networks for\\nQSAR predictions. arXiv:1406.1231. 26\\n728'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 21}, page_content='BIBLIOGRAPHY\\nDauphin, Y. and Bengio, Y. (2013). Stochastic ratio matching of RBMs for sparse\\nhigh-dimensional inputs. InNIPS26. NIPS Foundation. 617\\nDauphin, Y., Glorot, X., and Bengio, Y. (2011). Large-scale learning of embeddings with\\nreconstruction sampling. InICML’2011. 466\\nDauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. (2014).\\nIdentifying and attacking the saddle point problem in high-dimensional non-convex\\noptimization. In NIPS’2014. 282, 283, 284\\nDavis, A., Rubinstein, M., Wadhwa, N., Mysore, G., Durand, F., and Freeman, W. T.\\n(2014). The visual microphone: Passive recovery of sound from video.ACM Transactions\\non Graphics (Proc. SIGGRAPH), 33(4), 79:1–79:10. 447\\nDayan, P. (1990). Reinforcement comparison. InConnectionist Models: Proceedings of\\nthe 1990 Connectionist Summer School, San Mateo, CA. 688\\nDayan, P. and Hinton, G. E. (1996). Varieties of Helmholtz machine.Neural Networks,\\n9(8), 1385–1403. 689\\nDayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. (1995). The Helmholtz machine.\\nNeural computation, 7(5), 889–904. 689\\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q., Mao, M., Ranzato, M.,\\nSenior, A., Tucker, P., Yang, K., and Ng, A. Y. (2012). Large scale distributed deep\\nnetworks. InNIPS’2012. 25, 442\\nDean, T. and Kanazawa, K. (1989). A model for reasoning about persistence and causation.\\nComputational Intelligence, 5(3), 142–150. 659\\nDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990).\\nIndexing by latent semantic analysis.Journal of the American Society for Information\\nScience, 41(6), 391–407. 472, 477\\nDelalleau, O. and Bengio, Y. (2011). Shallow vs. deep sum-product networks. InNIPS.\\n18, 551\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A\\nLarge-Scale Hierarchical Image Database. InCVPR09. 19\\nDeng, J., Berg, A. C., Li, K., and Fei-Fei, L. (2010a). What does classifying more than\\n10,000 image categories tell us? InProceedings of the 11th European Conference on\\nComputer Vision: Part V, ECCV’10, pages 71–84, Berlin, Heidelberg. Springer-Verlag.\\n19\\nDeng, L. and Yu, D. (2014). Deep learning – methods and applications.Foundations and\\nTrends in Signal Processing. 455\\n729'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 22}, page_content='BIBLIOGRAPHY\\nDeng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and Hinton, G. (2010b). Binary\\ncoding of speech spectrograms using a deep auto-encoder. InInterspeech 2010, Makuhari,\\nChiba, Japan. 24\\nDenil, M., Bazzani, L., Larochelle, H., and de Freitas, N. (2012). Learning where to attend\\nwith deep architectures for image tracking.Neural Computation, 24(8), 2151–2184. 361\\nDenton, E., Chintala, S., Szlam, A., and Fergus, R. (2015). Deep generative image models\\nusing a Laplacian pyramid of adversarial networks.NIPS. 698, 699, 714\\nDesjardins, G. and Bengio, Y. (2008). Empirical evaluation of convolutional RBMs for\\nvision. Technical Report 1327, Département d’Informatique et de Recherche Opéra-\\ntionnelle, Université de Montréal. 679\\nDesjardins, G., Courville, A. C., Bengio, Y., Vincent, P., and Delalleau, O. (2010).\\nTempered Markov chain Monte Carlo for training of restricted Boltzmann machines. In\\nInternational Conference on Artiﬁcial Intelligence and Statistics, pages 145–152. 601,\\n612\\nDesjardins, G., Courville, A., and Bengio, Y. (2011). On tracking the partition function.\\nIn NIPS’2011. 628\\nDesjardins, G., Simonyan, K., Pascanu, R.,et al.(2015). Natural neural networks. In\\nAdvances in Neural Information Processing Systems, pages 2062–2070. 316\\nDevlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast\\nand robust neural network joint models for statistical machine translation. InProc.\\nACL’2014. 468\\nDevroye, L. (2013).Non-Uniform Random Variate Generation. SpringerLink : Bücher.\\nSpringer New York. 690\\nDiCarlo, J. J. (2013). Mechanisms underlying visual object recognition: Humans vs.\\nneurons vs. machines. NIPS Tutorial. 25, 360\\nDinh, L., Krueger, D., and Bengio, Y. (2014). NICE: Non-linear independent components\\nestimation. arXiv:1410.8516. 489\\nDonahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko,\\nK., and Darrell, T. (2014). Long-term recurrent convolutional networks for visual\\nrecognition and description. arXiv:1411.4389. 100\\nDonoho, D. L. and Grimes, C. (2003). Hessian eigenmaps: new locally linear embedding\\ntechniques for high-dimensional data. Technical Report 2003-08, Dept. Statistics,\\nStanford University. 160, 516\\nDosovitskiy, A., Springenberg, J. T., and Brox, T. (2015). Learning to generate chairs with\\nconvolutional neural networks. InProceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pages 1538–1546. 692, 701\\n730'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 23}, page_content='BIBLIOGRAPHY\\nDoya, K. (1993). Bifurcations of recurrent neural networks in gradient descent learning.\\nIEEE Transactions on Neural Networks, 1, 75–80. 396, 399\\nDreyfus, S. E. (1962). The numerical solution of variational problems. Journal of\\nMathematical Analysis and Applications, 5(1), 30–45. 221\\nDreyfus, S. E. (1973). The computational solution of optimal control problems with time\\nlag. IEEE Transactions on Automatic Control, 18(4), 383–385. 221\\nDrucker, H. and LeCun, Y. (1992). Improving generalisation performance using double\\nback-propagation. IEEE Transactions on Neural Networks, 3(6), 991–997. 269\\nDuchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online\\nlearning and stochastic optimization.Journal of Machine Learning Research. 303\\nDudik, M., Langford, J., and Li, L. (2011). Doubly robust policy evaluation and learning.\\nIn Proceedings of the 28th International Conference on Machine learning, ICML ’11.\\n477\\nDugas, C., Bengio, Y., Bélisle, F., and Nadeau, C. (2001). Incorporating second-order\\nfunctional knowledge for better option pricing. In T. Leen, T. Dietterich, and V. Tresp,\\neditors, Advances in Neural Information Processing Systems 13 (NIPS’00), pages\\n472–478. MIT Press. 66, 193\\nDziugaite, G. K., Roy, D. M., and Ghahramani, Z. (2015). Training generative neural net-\\nworks via maximum mean discrepancy optimization.arXiv preprint arXiv:1505.03906.\\n699\\nEl Hihi, S. and Bengio, Y. (1996). Hierarchical recurrent neural networks for long-term\\ndependencies. In NIPS’1995. 394, 403\\nElkahky, A. M., Song, Y., and He, X. (2015). A multi-view deep learning approach for\\ncross domain user modeling in recommendation systems. InProceedings of the 24th\\nInternational Conference on World Wide Web, pages 278–288. 475\\nElman, J. L. (1993). Learning and development in neural networks: The importance of\\nstarting small. Cognition, 48, 781–799. 324\\nErhan, D., Manzagol, P.-A., Bengio, Y., Bengio, S., and Vincent, P. (2009). The diﬃculty\\nof training deep architectures and the eﬀect of unsupervised pre-training. InProceedings\\nof AISTATS’2009. 197\\nErhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., and Bengio, S. (2010).\\nWhy does unsupervised pre-training help deep learning?J. Machine Learning Res.\\n527, 531, 532\\nFahlman, S. E., Hinton, G. E., and Sejnowski, T. J. (1983). Massively parallel architectures\\nfor AI: NETL, thistle, and Boltzmann machines. In Proceedings of the National\\nConference on Artiﬁcial Intelligence AAAI-83. 567, 651\\n731'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 24}, page_content='BIBLIOGRAPHY\\nFang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Dollár, P., Gao, J., He, X.,\\nMitchell, M., Platt, J. C., Zitnick, C. L., and Zweig, G. (2015). From captions to visual\\nconcepts and back. arXiv:1411.4952. 100\\nFarabet, C., LeCun, Y., Kavukcuoglu, K., Culurciello, E., Martini, B., Akselrod, P., and\\nTalay, S. (2011). Large-scale FPGA-based convolutional networks. In R. Bekkerman,\\nM. Bilenko, and J. Langford, editors,Scaling up Machine Learning: Parallel and\\nDistributed Approaches. Cambridge University Press. 521\\nFarabet, C., Couprie, C., Najman, L., and LeCun, Y. (2013). Learning hierarchical features\\nfor scene labeling.IEEE Transactions on Pattern Analysis and Machine Intelligence,\\n35(8), 1915–1929. 24, 197, 353\\nFei-Fei, L., Fergus, R., and Perona, P. (2006). One-shot learning of object categories.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 28(4), 594–611. 536\\nFinn, C., Tan, X. Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P. (2015). Learning\\nvisual feature spaces for robotic manipulation with deep spatial autoencoders.arXiv\\npreprint arXiv:1509.06113. 25\\nFisher, R. A. (1936). The use of multiple measurements in taxonomic problems.Annals\\nof Eugenics, 7, 179–188. 19, 103\\nFöldiák, P. (1989). Adaptive network for optimal linear feature extraction. InInternational\\nJoint Conference on Neural Networks (IJCNN), volume 1, pages 401–405, Washington\\n1989. IEEE, New York. 490\\nForcada, M. and Ñeco, R. (1997). Learning recursive distributed representations for\\nholistic computation. InBiological and Artiﬁcial Computation: From Neuroscience to\\nTechnology, pages 453–462. 468\\nFranzius, M., Sprekeler, H., and Wiskott, L. (2007). Slowness and sparseness lead to place,\\nhead-direction, and spatial-view cells. 491\\nFranzius, M., Wilbert, N., and Wiskott, L. (2008). Invariant object recognition with slow\\nfeature analysis. InArtiﬁcial Neural Networks-ICANN 2008, pages 961–970. Springer.\\n492\\nFrasconi, P., Gori, M., and Sperduti, A. (1997). On the eﬃcient classiﬁcation of data\\nstructures by neural networks. InProc. Int. Joint Conf. on Artiﬁcial Intelligence. 394,\\n396\\nFrasconi, P., Gori, M., and Sperduti, A. (1998). A general framework for adaptive\\nprocessing of data structures.IEEE Transactions on Neural Networks, 9(5), 768–786.\\n394, 396\\nFreund, Y. and Schapire, R. E. (1996a). Experiments with a new boosting algorithm. In\\nMachine Learning: Proceedings of Thirteenth International Conference, pages 148–156,\\nUSA. ACM. 255\\n732'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 25}, page_content='BIBLIOGRAPHY\\nFreund, Y. and Schapire, R. E. (1996b). Game theory, on-line prediction and boosting. In\\nProceedings of the Ninth Annual Conference on Computational Learning Theory, pages\\n325–332. 255\\nFrey, B. J. (1998).Graphical models for machine learning and digital communication.\\nMIT Press. 702\\nFrey, B. J., Hinton, G. E., and Dayan, P. (1996). Does the wake-sleep algorithm learn good\\ndensity estimators? In D. Touretzky, M. Mozer, and M. Hasselmo, editors,Advances\\nin Neural Information Processing Systems 8 (NIPS’95), pages 661–670. MIT Press,\\nCambridge, MA. 649\\nFrobenius, G. (1908). Über matrizen aus positiven elementen, s.B. Preuss. Akad. Wiss.\\nBerlin, Germany. 594\\nFukushima, K. (1975). Cognitron: A self-organizing multilayered neural network.Biological\\nCybernetics, 20, 121–136. 15, 222, 526\\nFukushima, K. (1980). Neocognitron: A self-organizing neural network model for a\\nmechanism of pattern recognition unaﬀected by shift in position.Biological Cybernetics,\\n36, 193–202. 15, 22, 23, 222, 361\\nGal, Y. and Ghahramani, Z. (2015). Bayesian convolutional neural networks with Bernoulli\\napproximate variational inference.arXiv preprint arXiv:1506.02158. 261\\nGallinari, P., LeCun, Y., Thiria, S., and Fogelman-Soulie, F. (1987). Memoires associatives\\ndistribuees. In Proceedings of COGNITIVA 87, Paris, La Villette. 511\\nGarcia-Duran, A., Bordes, A., Usunier, N., and Grandvalet, Y. (2015). Combining two\\nand three-way embeddings models for link prediction in knowledge bases.arXiv preprint\\narXiv:1506.00999. 479\\nGarofolo, J. S., Lamel, L. F., Fisher, W. M., Fiscus, J. G., and Pallett, D. S. (1993).\\nDarpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1.\\nNASA STI/Recon Technical Report N, 93, 27403. 454\\nGarson, J. (1900). The metric system of identiﬁcation of criminals, as used in Great\\nBritain and Ireland.The Journal of the Anthropological Institute of Great Britain and\\nIreland, (2), 177–227. 19\\nGers, F. A., Schmidhuber, J., and Cummins, F. (2000). Learning to forget: Continual\\nprediction with LSTM.Neural computation, 12(10), 2451–2471. 404, 408\\nGhahramani, Z. and Hinton, G. E. (1996). The EM algorithm for mixtures of factor\\nanalyzers. Technical Report CRG-TR-96-1, Dpt. of Comp. Sci., Univ. of Toronto. 485\\nGillick, D., Brunk, C., Vinyals, O., and Subramanya, A. (2015). Multilingual language\\nprocessing from bytes.arXiv preprint arXiv:1512.00103. 472\\n733'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 26}, page_content='BIBLIOGRAPHY\\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. (2015). Region-based convolutional\\nnetworks for accurate object detection and segmentation. 421\\nGiudice, M. D., Manera, V., and Keysers, C. (2009). Programmed to learn? The ontogeny\\nof mirror neurons.Dev. Sci., 12(2), 350––363. 653\\nGlorot, X. and Bengio, Y. (2010). Understanding the diﬃculty of training deep feedforward\\nneural networks. InAISTATS’2010. 299\\nGlorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectiﬁer neural networks. In\\nAISTATS’2011. 15, 171, 193, 222\\nGlorot, X., Bordes, A., and Bengio, Y. (2011b). Domain adaptation for large-scale\\nsentiment classiﬁcation: A deep learning approach. InICML’2011. 504, 535\\nGoldberger, J., Roweis, S., Hinton, G. E., and Salakhutdinov, R. (2005). Neighbourhood\\ncomponents analysis. In L. Saul, Y. Weiss, and L. Bottou, editors,Advances in Neural\\nInformation Processing Systems 17 (NIPS’04). MIT Press. 113\\nGong, S., McKenna, S., and Psarrou, A. (2000).Dynamic Vision: From Images to Face\\nRecognition. Imperial College Press. 161, 516\\nGoodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Measuring invariances in deep\\nnetworks. InNIPS’2009, pages 646–654. 252\\nGoodfellow, I., Koenig, N., Muja, M., Pantofaru, C., Sorokin, A., and Takayama, L. (2010).\\nHelp me help you: Interfaces for personal robots. InProc. of Human Robot Interaction\\n(HRI), Osaka, Japan. ACM Press, ACM Press. 98\\nGoodfellow, I. J. (2010). Technical report: Multidimensional, downsampled convolution\\nfor autoencoders. Technical report, Université de Montréal. 350\\nGoodfellow, I. J. (2014). On distinguishability criteria for estimating generative models.\\nIn International Conference on Learning Representations, Workshops Track. 620, 697\\nGoodfellow, I. J., Courville, A., and Bengio, Y. (2011). Spike-and-slab sparse coding\\nfor unsupervised feature discovery. In NIPS Workshop on Challenges in Learning\\nHierarchical Models. 530, 536\\nGoodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013a).\\nMaxout networks. In S. Dasgupta and D. McAllester, editors,ICML’13, pages 1319–\\n1327. 190, 261, 338, 359, 450\\nGoodfellow, I. J., Mirza, M., Courville, A., and Bengio, Y. (2013b). Multi-prediction deep\\nBoltzmann machines. InNIPS26. NIPS Foundation. 98, 615, 668, 669, 670, 671, 672,\\n695\\n734'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 27}, page_content='BIBLIOGRAPHY\\nGoodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R.,\\nBergstra, J., Bastien, F., and Bengio, Y. (2013c). Pylearn2: a machine learning research\\nlibrary. arXiv preprint arXiv:1308.4214. 25, 441\\nGoodfellow, I. J., Courville, A., and Bengio, Y. (2013d). Scaling up spike-and-slab models\\nfor unsupervised feature learning.IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 35(8), 1902–1914. 493, 494, 495, 647, 679\\nGoodfellow, I. J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y. (2014a). An empirical\\ninvestigation of catastrophic forgeting in gradient-based neural networks. InICLR’2014.\\n191\\nGoodfellow, I. J., Shlens, J., and Szegedy, C. (2014b). Explaining and harnessing adver-\\nsarial examples. CoRR, abs/1412.6572. 265, 266, 269, 553, 554\\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\\nCourville, A., and Bengio, Y. (2014c). Generative adversarial networks. InNIPS’2014.\\n542, 685, 696, 698, 701\\nGoodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. (2014d). Multi-digit\\nnumber recognition from Street View imagery using deep convolutional neural networks.\\nIn International Conference on Learning Representations. 25, 99, 197, 198, 199, 385,\\n417, 444\\nGoodfellow, I. J., Vinyals, O., and Saxe, A. M. (2015). Qualitatively characterizing neural\\nnetwork optimization problems. InInternational Conference on Learning Representa-\\ntions. 282, 283, 284, 287\\nGoodman, J. (2001). Classes for fast maximum entropy training. In International\\nConference on Acoustics, Speech and Signal Processing (ICASSP), Utah. 462\\nGori, M. and Tesi, A. (1992). On the problem of local minima in backpropagation.IEEE\\nTransactions on Pattern Analysis and Machine Intelligence, PAMI-14(1), 76–86. 282\\nGosset, W. S. (1908). The probable error of a mean.Biometrika, 6(1), 1–25. Originally\\npublished under the pseudonym “Student”. 19\\nGouws, S., Bengio, Y., and Corrado, G. (2014). BilBOWA: Fast bilingual distributed\\nrepresentations without word alignments. Technical report, arXiv:1410.2455. 472, 537\\nGraf, H. P. and Jackel, L. D. (1989). Analog electronic neural network circuits.Circuits\\nand Devices Magazine, IEEE, 5(4), 44–49. 446\\nGraves, A. (2011). Practical variational inference for neural networks. InNIPS’2011. 238\\nGraves, A. (2012).Supervised Sequence Labelling with Recurrent Neural Networks. Studies\\nin Computational Intelligence. Springer. 368, 388, 407, 455\\n735'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 28}, page_content='BIBLIOGRAPHY\\nGraves, A. (2013). Generating sequences with recurrent neural networks. Technical report,\\narXiv:1308.0850. 186, 404, 411, 415\\nGraves, A. and Jaitly, N. (2014). Towards end-to-end speech recognition with recurrent\\nneural networks. InICML’2014. 404\\nGraves, A. and Schmidhuber, J. (2005). Framewise phoneme classiﬁcation with bidirec-\\ntional LSTM and other neural network architectures.Neural Networks, 18(5), 602–610.\\n388\\nGraves, A. and Schmidhuber, J. (2009). Oﬄine handwriting recognition with multidi-\\nmensional recurrent neural networks. In D. Koller, D. Schuurmans, Y. Bengio, and\\nL. Bottou, editors,NIPS’2008, pages 545–552. 388\\nGraves, A., Fernández, S., Gomez, F., and Schmidhuber, J. (2006). Connectionist temporal\\nclassiﬁcation: Labelling unsegmented sequence data with recurrent neural networks. In\\nICML’2006, pages 369–376, Pittsburgh, USA. 455\\nGraves, A., Liwicki, M., Bunke, H., Schmidhuber, J., and Fernández, S. (2008). Uncon-\\nstrained on-line handwriting recognition with recurrent neural networks. In J. Platt,\\nD. Koller, Y. Singer, and S. Roweis, editors,NIPS’2007, pages 577–584. 388\\nGraves, A., Liwicki, M., Fernández, S., Bertolami, R., Bunke, H., and Schmidhuber, J.\\n(2009). A novel connectionist system for unconstrained handwriting recognition.Pattern\\nAnalysis and Machine Intelligence, IEEE Transactions on, 31(5), 855–868. 404\\nGraves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent\\nneural networks. InICASSP’2013, pages 6645–6649. 388, 393, 394, 404, 406, 407, 455\\nGraves, A., Wayne, G., and Danihelka, I. (2014a). Neural Turing machines.\\narXiv:1410.5401. 25\\nGraves, A., Wayne, G., and Danihelka, I. (2014b). Neural Turing machines.arXiv preprint\\narXiv:1410.5401. 412\\nGrefenstette, E., Hermann, K. M., Suleyman, M., and Blunsom, P. (2015). Learning to\\ntransduce with unbounded memory. InNIPS’2015. 412\\nGreﬀ, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., and Schmidhuber, J. (2015).\\nLSTM: a search space odyssey.arXiv preprint arXiv:1503.04069. 408\\nGregor, K. and LeCun, Y. (2010a). Emergence of complex-like cells in a temporal product\\nnetwork with local receptive ﬁelds. Technical report, arXiv:1006.0448. 346\\nGregor, K. and LeCun, Y. (2010b). Learning fast approximations of sparse coding. In\\nL. Bottou and M. Littman, editors,Proceedings of the Twenty-seventh International\\nConference on Machine Learning (ICML-10). ACM. 650\\n736'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 29}, page_content='BIBLIOGRAPHY\\nGregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. (2014). Deep\\nautoregressivenetworks. InInternational Conference on Machine Learning (ICML’2014).\\n690\\nGregor, K., Danihelka, I., Graves, A., and Wierstra, D. (2015). DRAW: A recurrent neural\\nnetwork for image generation.arXiv preprint arXiv:1502.04623. 694\\nGretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., and Smola, A. (2012). A\\nkernel two-sample test.The Journal of Machine Learning Research, 13(1), 723–773.\\n700\\nGülçehre, Ç. and Bengio, Y. (2013). Knowledge matters: Importance of prior information\\nfor optimization. InInternational Conference on Learning Representations (ICLR’2013).\\n25\\nGuo, H. and Gelfand, S. B. (1992). Classiﬁcation trees with neural network feature\\nextraction. Neural Networks, IEEE Transactions on, 3(6), 923–933. 445\\nGupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. (2015). Deep learning\\nwith limited numerical precision.CoRR, abs/1502.02551. 447\\nGutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation: A new estima-\\ntion principle for unnormalized statistical models. InProceedings of The Thirteenth\\nInternational Conference on Artiﬁcial Intelligence and Statistics (AISTATS’10). 618\\nHadsell, R., Sermanet, P., Ben, J., Erkan, A., Han, J., Muller, U., and LeCun, Y.\\n(2007). Online learning for oﬀroad robots: Spatial label propagation to learn long-range\\ntraversability. InProceedings of Robotics: Science and Systems, Atlanta, GA, USA. 448\\nHajnal, A., Maass, W., Pudlak, P., Szegedy, M., and Turan, G. (1993). Threshold circuits\\nof bounded depth.J. Comput. System. Sci., 46, 129–154. 196\\nHåstad, J. (1986). Almost optimal lower bounds for small depth circuits. InProceedings\\nof the 18th annual ACM Symposium on Theory of Computing, pages 6–20, Berkeley,\\nCalifornia. ACM Press. 196\\nHåstad, J. and Goldmann, M. (1991). On the power of small-depth threshold circuits.\\nComputational Complexity, 1, 113–129. 196\\nHastie, T., Tibshirani, R., and Friedman, J. (2001).The elements of statistical learning:\\ndata mining, inference and prediction. Springer Series in Statistics. Springer Verlag.\\n142\\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectiﬁers: Surpassing\\nhuman-level performance on ImageNet classiﬁcation.arXiv preprint arXiv:1502.01852.\\n24, 190\\nHebb, D. O. (1949).The Organization of Behavior. Wiley, New York. 13, 16, 653\\n737'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 30}, page_content='BIBLIOGRAPHY\\nHenaﬀ, M., Jarrett, K., Kavukcuoglu, K., and LeCun, Y. (2011). Unsupervised learning\\nof sparse features for scalable audio classiﬁcation. InISMIR’11. 521\\nHenderson, J. (2003). Inducing history representations for broad coverage statistical\\nparsing. In HLT-NAACL, pages 103–110. 473\\nHenderson, J. (2004). Discriminative training of a neural network statistical parser. In\\nProceedings of the 42nd Annual Meeting on Association for Computational Linguistics,\\npage 95. 473\\nHenniges, M., Puertas, G., Bornschein, J., Eggert, J., and Lücke, J. (2010). Binary sparse\\ncoding. In Latent Variable Analysis and Signal Separation, pages 450–457. Springer.\\n638\\nHerault, J. and Ans, B. (1984). Circuits neuronaux à synapses modiﬁables: Décodage de\\nmessages composites par apprentissage non supervisé.Comptes Rendus de l’Académie\\ndes Sciences, 299(III-13), 525––528. 487\\nHinton, G. (2012). Neural networks for machine learning. Coursera, video lectures. 303\\nHinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V.,\\nNguyen, P., Sainath, T., and Kingsbury, B. (2012a). Deep neural networks for acoustic\\nmodeling in speech recognition.IEEE Signal Processing Magazine, 29(6), 82–97. 24,\\n454\\nHinton, G., Vinyals, O., and Dean, J. (2015). Distilling the knowledge in a neural network.\\narXiv preprint arXiv:1503.02531. 443\\nHinton, G. E. (1989). Connectionist learning procedures. Artiﬁcial Intelligence, 40,\\n185–234. 490\\nHinton, G.E.(1990). Mappingpart-wholehierarchiesintoconnectionistnetworks. Artiﬁcial\\nIntelligence, 46(1), 47–75. 412\\nHinton, G. E. (1999). Products of experts. InICANN’1999. 567\\nHinton, G. E. (2000). Training products of experts by minimizing contrastive divergence.\\nTechnical Report GCNU TR 2000-004, Gatsby Unit, University College London. 608,\\n673\\nHinton, G. E. (2006). To recognize shapes, ﬁrst learn to generate images. Technical Report\\nUTML TR 2006-003, University of Toronto. 526, 592\\nHinton, G. E. (2007a). How to do backpropagation in a brain. Invited talk at the\\nNIPS’2007 Deep Learning Workshop. 653\\nHinton, G. E. (2007b). Learning multiple layers of representation.Trends in cognitive\\nsciences, 11(10), 428–434. 657\\n738'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 31}, page_content='BIBLIOGRAPHY\\nHinton, G. E. (2010). A practical guide to training restricted Boltzmann machines.\\nTechnical Report UTML TR 2010-003, Department of Computer Science, University of\\nToronto. 608\\nHinton, G. E. and Ghahramani, Z. (1997). Generative models for discovering sparse\\ndistributed representations. Philosophical Transactions of the Royal Society of London.\\n144\\nHinton, G. E. and McClelland, J. L. (1988). Learning representations by recirculation. In\\nNIPS’1987, pages 358–366. 499\\nHinton, G. E. and Roweis, S. (2003). Stochastic neighbor embedding. InNIPS’2002. 516\\nHinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with\\nneural networks.Science, 313(5786), 504–507. 506, 522, 526, 527, 531\\nHinton, G. E. and Sejnowski, T. J. (1986). Learning and relearning in Boltzmann machines.\\nIn D. E. Rumelhart and J. L. McClelland, editors,Parallel Distributed Processing,\\nvolume 1, chapter 7, pages 282–317. MIT Press, Cambridge. 567, 651\\nHinton, G. E. and Sejnowski, T. J. (1999).Unsupervised learning: foundations of neural\\ncomputation. MIT press. 539\\nHinton, G. E. and Shallice, T. (1991). Lesioning an attractor network: investigations of\\nacquired dyslexia. Psychological review, 98(1), 74. 13\\nHinton, G. E. and Zemel, R. S. (1994). Autoencoders, minimum description length, and\\nHelmholtz free energy. InNIPS’1993. 499\\nHinton, G. E., Sejnowski, T. J., and Ackley, D. H. (1984). Boltzmann machines: Constraint\\nsatisfactionnetworksthatlearn. TechnicalReportTR-CMU-CS-84-119, Carnegie-Mellon\\nUniversity, Dept. of Computer Science. 567, 651\\nHinton, G. E., McClelland, J., and Rumelhart, D. (1986). Distributed representations.\\nIn D. E. Rumelhart and J. L. McClelland, editors,Parallel Distributed Processing:\\nExplorations in the Microstructure of Cognition, volume 1, pages 77–109. MIT Press,\\nCambridge. 16, 221, 524\\nHinton, G. E., Revow, M., and Dayan, P. (1995a). Recognizing handwritten digits using\\nmixtures of linear models. In G. Tesauro, D. Touretzky, and T. Leen, editors,Advances\\nin Neural Information Processing Systems 7 (NIPS’94), pages 1015–1022. MIT Press,\\nCambridge, MA. 485\\nHinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995b). The wake-sleep algorithm\\nfor unsupervised neural networks.Science, 268, 1558–1161. 501, 649\\nHinton, G. E., Dayan, P., and Revow, M. (1997). Modelling the manifolds of images of\\nhandwritten digits. IEEE Transactions on Neural Networks, 8, 65–74. 496\\n739'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 32}, page_content='BIBLIOGRAPHY\\nHinton, G. E., Welling, M., Teh, Y. W., and Osindero, S. (2001). A new view of ICA. In\\nProceedings of 3rd International Conference on Independent Component Analysis and\\nBlind Signal Separation (ICA’01), pages 746–751, San Diego, CA. 487\\nHinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief\\nnets. Neural Computation, 18, 1527–1554. 13, 18, 23, 141, 526, 527, 657, 658\\nHinton, G. E., Deng, L., Yu, D., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A.,\\nVanhoucke, V., Nguyen, P., Sainath, T. N., and Kingsbury, B. (2012b). Deep neural\\nnetworks for acoustic modeling in speech recognition: The shared views of four research\\ngroups. IEEE Signal Process. Mag., 29(6), 82–97. 99\\nHinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012c).\\nImproving neural networks by preventing co-adaptation of feature detectors. Technical\\nreport, arXiv:1207.0580. 235, 259, 264\\nHinton, G. E., Vinyals, O., and Dean, J. (2014). Dark knowledge. Invited talk at the\\nBayLearn Bay Area Machine Learning Symposium. 443\\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma\\nthesis, T.U. München. 17, 396, 398\\nHochreiter, S. and Schmidhuber, J. (1995). Simplifying neural nets by discovering ﬂat\\nminima. InAdvances in Neural Information Processing Systems 7, pages 529–536. MIT\\nPress. 239\\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory.Neural Computation,\\n9(8), 1735–1780. 17, 404, 407\\nHochreiter, S., Bengio, Y., and Frasconi, P. (2001). Gradient ﬂow in recurrent nets: the\\ndiﬃculty of learning long-term dependencies. In J. Kolen and S. Kremer, editors,Field\\nGuide to Dynamical Recurrent Networks. IEEE Press. 407\\nHoli, J. L. and Hwang, J.-N. (1993). Finite precision error analysis of neural network\\nhardware implementations.Computers, IEEE Transactions on, 42(3), 281–290. 446\\nHolt, J. L. and Baker, T. E. (1991). Back propagation simulations using limited preci-\\nsion calculations. In Neural Networks, 1991., IJCNN-91-Seattle International Joint\\nConference on, volume 2, pages 121–126. IEEE. 446\\nHornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are\\nuniversal approximators.Neural Networks, 2, 359–366. 194\\nHornik, K., Stinchcombe, M., and White, H. (1990). Universal approximation of an\\nunknown mapping and its derivatives using multilayer feedforward networks.Neural\\nnetworks, 3(5), 551–560. 194\\nHsu, F.-H. (2002).Behind Deep Blue: Building the Computer That Defeated the World\\nChess Champion. Princeton University Press, Princeton, NJ, USA. 2\\n740'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 33}, page_content='BIBLIOGRAPHY\\nHuang, F. and Ogata, Y. (2002). Generalized pseudo-likelihood estimates for Markov\\nrandom ﬁelds on lattice.Annals of the Institute of Statistical Mathematics, 54(1), 1–18.\\n614\\nHuang, P.-S., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. (2013). Learning deep\\nstructured semantic models for web search using clickthrough data. InProceedings of\\nthe 22nd ACM international conference on Conference on information & knowledge\\nmanagement, pages 2333–2338. ACM. 475\\nHubel, D. and Wiesel, T. (1968). Receptive ﬁelds and functional architecture of monkey\\nstriate cortex. Journal of Physiology (London), 195, 215–243. 358\\nHubel, D. H. and Wiesel, T. N. (1959). Receptive ﬁelds of single neurons in the cat’s\\nstriate cortex. Journal of Physiology, 148, 574–591. 358\\nHubel, D. H. and Wiesel, T. N. (1962). Receptive ﬁelds, binocular interaction, and\\nfunctional architecture in the cat’s visual cortex.Journal of Physiology (London), 160,\\n106–154. 358\\nHuszar, F. (2015). How (not) to train your generative model: schedule sampling, likelihood,\\nadversary? arXiv:1511.05101. 694\\nHutter, F., Hoos, H., and Leyton-Brown, K. (2011). Sequential model-based optimization\\nfor general algorithm conﬁguration. InLION-5. Extended version as UBC Tech report\\nTR-2010-10. 430\\nHyotyniemi, H. (1996). Turing machines are recurrent neural networks. InSTeP’96, pages\\n13–24. 372\\nHyvärinen, A. (1999). Survey on independent component analysis.Neural Computing\\nSurveys, 2, 94–128. 487\\nHyvärinen, A.(2005). Estimationofnon-normalizedstatisticalmodelsusingscorematching.\\nJournal of Machine Learning Research, 6, 695–709. 509, 615\\nHyvärinen, A. (2007a). Connections between score matching, contrastive divergence,\\nand pseudolikelihood for continuous-valued variables.IEEE Transactions on Neural\\nNetworks, 18, 1529–1531. 616\\nHyvärinen, A. (2007b). Some extensions of score matching.Computational Statistics and\\nData Analysis, 51, 2499–2512. 616\\nHyvärinen, A. and Hoyer, P. O. (1999). Emergence of topography and complex cell\\nproperties from natural images using extensions of ica. InNIPS, pages 827–833. 489\\nHyvärinen, A. and Pajunen, P. (1999). Nonlinear independent component analysis:\\nExistence and uniqueness results.Neural Networks, 12(3), 429–439. 489\\n741'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 34}, page_content='BIBLIOGRAPHY\\nHyvärinen, A., Karhunen, J., and Oja, E. (2001a).Independent Component Analysis.\\nWiley-Interscience. 487\\nHyvärinen, A., Hoyer, P. O., and Inki, M. O. (2001b). Topographic independent component\\nanalysis. Neural Computation, 13(7), 1527–1558. 489\\nHyvärinen, A., Hurri, J., and Hoyer, P. O. (2009).Natural Image Statistics: A probabilistic\\napproach to early computational vision. Springer-Verlag. 364\\nIba, Y. (2001). Extended ensemble Monte Carlo.International Journal of Modern Physics,\\nC12, 623–656. 601\\nInayoshi, H. and Kurita, T. (2005). Improved generalization by adding both auto-\\nassociation and hidden-layer noise to neural-network-based-classiﬁers.IEEE Workshop\\non Machine Learning for Signal Processing, pages 141—-146. 511\\nIoﬀe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training\\nby reducing internal covariate shift. 98, 313, 316\\nJacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation.\\nNeural networks, 1(4), 295–307. 303\\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive mixtures\\nof local experts.Neural Computation, 3, 79–87. 185, 445\\nJaeger, H. (2003). Adaptive nonlinear system identiﬁcation with echo state networks. In\\nAdvances in Neural Information Processing Systems 15. 399\\nJaeger, H. (2007a). Discovering multiscale dynamical features with hierarchical echo state\\nnetworks. Technical report, Jacobs University. 394\\nJaeger, H. (2007b). Echo state network.Scholarpedia, 2(9), 2330. 399\\nJaeger, H. (2012). Long short-term memory in echo state networks: Details of a simulation\\nstudy. Technical report, Technical report, Jacobs University Bremen. 400\\nJaeger, H. and Haas, H. (2004). Harnessing nonlinearity: Predicting chaotic systems and\\nsaving energy in wireless communication.Science, 304(5667), 78–80. 23, 399\\nJaeger, H., Lukosevicius, M., Popovici, D., and Siewert, U. (2007). Optimization and\\napplications of echo state networks with leaky- integrator neurons.Neural Networks,\\n20(3), 335–352. 403\\nJain, V., Murray, J. F., Roth, F., Turaga, S., Zhigulin, V., Briggman, K. L., Helmstaedter,\\nM. N., Denk, W., and Seung, H. S. (2007). Supervised learning of image restoration\\nwith convolutional networks. In Computer Vision, 2007. ICCV 2007. IEEE 11th\\nInternational Conference on, pages 1–8. IEEE. 352\\n742'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 35}, page_content='BIBLIOGRAPHY\\nJaitly, N. and Hinton, G. (2011). Learning a better representation of speech soundwaves\\nusing restricted Boltzmann machines. In Acoustics, Speech and Signal Processing\\n(ICASSP), 2011 IEEE International Conference on, pages 5884–5887. IEEE. 453\\nJaitly, N. and Hinton, G. E. (2013). Vocal tract length perturbation (VTLP) improves\\nspeech recognition. InICML’2013. 237\\nJarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best\\nmulti-stage architecture for object recognition? InICCV’09. 15, 22, 23, 171, 190, 222,\\n357, 358, 521\\nJarzynski, C. (1997). Nonequilibrium equality for free energy diﬀerences.Phys. Rev. Lett.,\\n78, 2690–2693. 623, 626\\nJaynes, E. T. (2003).Probability Theory: The Logic of Science. Cambridge University\\nPress. 51\\nJean, S., Cho, K., Memisevic, R., and Bengio, Y. (2014). On using very large target\\nvocabulary for neural machine translation. arXiv:1412.2007. 469, 470\\nJelinek, F. and Mercer, R. L. (1980). Interpolated estimation of Markov source parameters\\nfrom sparse data. In E. S. Gelsema and L. N. Kanal, editors,Pattern Recognition in\\nPractice. North-Holland, Amsterdam. 457, 468\\nJia, Y. (2013). Caﬀe: An open source convolutional architecture for fast feature embedding.\\nhttp://caffe.berkeleyvision.org/. 25, 209\\nJia, Y., Huang, C., and Darrell, T. (2012). Beyond spatial pyramids: Receptive ﬁeld\\nlearning for pooled image features. In Computer Vision and Pattern Recognition\\n(CVPR), 2012 IEEE Conference on, pages 3370–3377. IEEE. 339\\nJim, K.-C., Giles, C. L., and Horne, B. G. (1996). An analysis of noise in recurrent neural\\nnetworks: convergence and generalization.IEEE Transactions on Neural Networks,\\n7(6), 1424–1438. 238\\nJordan, M. I. (1998).Learning in Graphical Models. Kluwer, Dordrecht, Netherlands. 17\\nJoulin, A. and Mikolov, T. (2015). Inferring algorithmic patterns with stack-augmented\\nrecurrent nets.arXiv preprint arXiv:1503.01007. 412\\nJozefowicz, R., Zaremba, W., and Sutskever, I. (2015). An empirical evaluation of recurrent\\nnetwork architectures. InICML’2015. 302, 407, 408\\nJudd, J. S. (1989).Neural Network Design and the Complexity of Learning. MIT press.\\n289\\nJutten, C. and Herault, J. (1991). Blind separation of sources, part I: an adaptive\\nalgorithm based on neuromimetic architecture.Signal Processing, 24, 1–10. 487\\n743'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 36}, page_content='BIBLIOGRAPHY\\nKahou, S. E., Pal, C., Bouthillier, X., Froumenty, P., Gülçehre, c., Memisevic, R., Vincent,\\nP., Courville, A., Bengio, Y., Ferrari, R. C., Mirza, M., Jean, S., Carrier, P. L., Dauphin,\\nY., Boulanger-Lewandowski, N., Aggarwal, A., Zumer, J., Lamblin, P., Raymond,\\nJ.-P., Desjardins, G., Pascanu, R., Warde-Farley, D., Torabi, A., Sharma, A., Bengio,\\nE., Côté, M., Konda, K. R., and Wu, Z. (2013). Combining modality speciﬁc deep\\nneural networks for emotion recognition in video. InProceedings of the 15th ACM on\\nInternational Conference on Multimodal Interaction. 197\\nKalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In\\nEMNLP’2013. 469, 470\\nKalchbrenner, N., Danihelka, I., and Graves, A. (2015). Grid long short-term memory.\\narXiv preprint arXiv:1507.01526. 390\\nKamyshanska, H. and Memisevic, R. (2015). The potential energy of an autoencoder.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence. 511\\nKarpathy, A. and Li, F.-F. (2015). Deep visual-semantic alignments for generating image\\ndescriptions. In CVPR’2015. arXiv:1412.2306. 100\\nKarpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., and Fei-Fei, L. (2014).\\nLarge-scale video classiﬁcation with convolutional neural networks. InCVPR. 19\\nKarush, W. (1939).Minima of Functions of Several Variables with Inequalities as Side\\nConstraints. Master’s thesis, Dept. of Mathematics, Univ. of Chicago. 93\\nKatz, S. M. (1987). Estimation of probabilities from sparse data for the language model\\ncomponent of a speech recognizer.IEEE Transactions on Acoustics, Speech, and Signal\\nProcessing, ASSP-35(3), 400–401. 457, 468\\nKavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008). Fast inference in sparse coding\\nalgorithms with applications to object recognition. Technical report, Computational and\\nBiological Learning Lab, Courant Institute, NYU. Tech Report CBLL-TR-2008-12-01.\\n521\\nKavukcuoglu, K., Ranzato, M.-A., Fergus, R., and LeCun, Y. (2009). Learning invariant\\nfeatures through topographic ﬁlter maps. InCVPR’2009. 521\\nKavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Mathieu, M., and LeCun, Y.\\n(2010). Learning convolutional feature hierarchies for visual recognition. InNIPS’2010.\\n358, 521\\nKelley, H. J. (1960). Gradient theory of optimal ﬂight paths.ARS Journal, 30(10),\\n947–954. 221\\nKhan, F., Zhu, X., and Mutlu, B. (2011). How do humans teach: On curriculum learning\\nand teaching dimension. InAdvances in Neural Information Processing Systems 24\\n(NIPS’11), pages 1449–1457. 324\\n744'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 37}, page_content='BIBLIOGRAPHY\\nKim, S. K., McAfee, L. C., McMahon, P. L., and Olukotun, K. (2009). A highly scalable\\nrestricted Boltzmann machine FPGA implementation. InField Programmable Logic\\nand Applications, 2009. FPL 2009. International Conference on, pages 367–372. IEEE.\\n446\\nKindermann, R. (1980).Markov Random Fields and Their Applications (Contemporary\\nMathematics ; V. 1). American Mathematical Society. 563\\nKingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization.arXiv\\npreprint arXiv:1412.6980. 305\\nKingma, D. and LeCun, Y. (2010). Regularized estimation of image statistics by score\\nmatching. InNIPS’2010. 509, 618\\nKingma, D., Rezende, D., Mohamed, S., and Welling, M. (2014). Semi-supervised learning\\nwith deep generative models. InNIPS’2014. 421\\nKingma, D. P. (2013). Fast gradient-based inference with continuous latent variable\\nmodels in auxiliary form. Technical report, arxiv:1306.0733. 650, 685, 693\\nKingma, D. P. and Welling, M. (2014a). Auto-encoding variational bayes. InProceedings\\nof the International Conference on Learning Representations (ICLR). 685, 696\\nKingma, D. P. and Welling, M. (2014b). Eﬃcient gradient-based inference through\\ntransformations between bayes nets and neural nets. Technical report, arxiv:1402.0480.\\n685\\nKirkpatrick, S., Jr., C. D. G., , and Vecchi, M. P. (1983). Optimization by simulated\\nannealing. Science, 220, 671–680. 323\\nKiros, R., Salakhutdinov, R., and Zemel, R. (2014a). Multimodal neural language models.\\nIn ICML’2014. 100\\nKiros, R., Salakhutdinov, R., and Zemel, R. (2014b). Unifying visual-semantic embeddings\\nwith multimodal neural language models.arXiv:1411.2539 [cs.LG]. 100, 404\\nKlementiev, A., Titov, I., and Bhattarai, B. (2012). Inducing crosslingual distributed\\nrepresentations of words. InProceedings of COLING 2012. 472, 537\\nKnowles-Barley, S., Jones, T. R., Morgan, J., Lee, D., Kasthuri, N., Lichtman, J. W., and\\nPﬁster, H. (2014). Deep learning for the connectome.GPU Technology Conference. 26\\nKoller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and\\nTechniques. MIT Press. 580, 592, 643\\nKonig, Y., Bourlard, H., and Morgan, N. (1996). REMAP: Recursive estimation and\\nmaximization of a posteriori probabilities – application to transition-based connectionist\\nspeech recognition. In D. Touretzky, M. Mozer, and M. Hasselmo, editors,Advances in\\nNeural Information Processing Systems 8 (NIPS’95). MIT Press, Cambridge, MA. 454\\n745'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 38}, page_content='BIBLIOGRAPHY\\nKoren, Y. (2009). The BellKor solution to the Netﬂix grand prize. 255, 475\\nKotzias, D., Denil, M., de Freitas, N., and Smyth, P. (2015). From group to individual\\nlabels using deep features. InACM SIGKDD. 104\\nKoutnik, J., Greﬀ, K., Gomez, F., and Schmidhuber, J. (2014). A clockwork RNN. In\\nICML’2014. 403\\nKočiský, T., Hermann, K. M., and Blunsom, P. (2014). Learning Bilingual Word Repre-\\nsentations by Marginalizing Alignments. InProceedings of ACL. 470\\nKrause, O., Fischer, A., Glasmachers, T., and Igel, C. (2013). Approximation properties\\nof DBNs with binary hidden units and real-valued visible units. InICML’2013. 551\\nKrizhevsky, A. (2010). Convolutional deep belief networks on CIFAR-10. Technical report,\\nUniversity of Toronto. Unpublished Manuscript: http://www.cs.utoronto.ca/ kriz/conv-\\ncifar10-aug2010.pdf. 441\\nKrizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny\\nimages. Technical report, University of Toronto. 19, 558\\nKrizhevsky, A. and Hinton, G. E. (2011). Using very deep autoencoders for content-based\\nimage retrieval. InESANN. 523\\nKrizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classiﬁcation with deep\\nconvolutional neural networks. InNIPS’2012. 22, 23, 24, 98, 197, 365, 449, 453\\nKrueger, K. A. and Dayan, P. (2009). Flexible shaping: how learning in small steps helps.\\nCognition, 110, 380–394. 324\\nKuhn, H. W. and Tucker, A. W. (1951). Nonlinear programming. InProceedings of the\\nSecond Berkeley Symposium on Mathematical Statistics and Probability, pages 481–492,\\nBerkeley, Calif. University of California Press. 93\\nKumar, A., Irsoy, O., Su, J., Bradbury, J., English, R., Pierce, B., Ondruska, P., Iyyer,\\nM., Gulrajani, I., and Socher, R. (2015). Ask me anything: Dynamic memory networks\\nfor natural language processing.arXiv:1506.07285. 412, 480\\nKumar, M. P., Packer, B., and Koller, D. (2010). Self-paced learning for latent variable\\nmodels. In NIPS’2010. 324\\nLang, K. J. and Hinton, G. E. (1988). The development of the time-delay neural network\\narchitecture for speech recognition. Technical Report CMU-CS-88-152, Carnegie-Mellon\\nUniversity. 361, 368, 402\\nLang, K. J., Waibel, A. H., and Hinton, G. E. (1990). A time-delay neural network\\narchitecture for isolated word recognition.Neural networks, 3(1), 23–43. 368\\n746'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 39}, page_content='BIBLIOGRAPHY\\nLangford, J. and Zhang, T. (2008). The epoch-greedy algorithm for contextual multi-armed\\nbandits. In NIPS’2008, pages 1096––1103. 476\\nLappalainen, H., Giannakopoulos, X., Honkela, A., and Karhunen, J. (2000). Nonlinear\\nindependent component analysis using ensemble learning: Experiments and discussion.\\nIn Proc. ICA. Citeseer. 489\\nLarochelle, H. and Bengio, Y. (2008). Classiﬁcation using discriminative restricted\\nBoltzmann machines. InICML’2008. 240, 252, 528, 683, 712\\nLarochelle, H. and Hinton, G. E. (2010). Learning to combine foveal glimpses with a\\nthird-order Boltzmann machine. InAdvances in Neural Information Processing Systems\\n23, pages 1243–1251. 361\\nLarochelle, H. and Murray, I. (2011). The Neural Autoregressive Distribution Estimator.\\nIn AISTATS’2011. 702, 705, 706\\nLarochelle, H., Erhan, D., and Bengio, Y. (2008). Zero-data learning of new tasks. In\\nAAAI Conference on Artiﬁcial Intelligence. 537\\nLarochelle, H., Bengio, Y., Louradour, J., and Lamblin, P. (2009). Exploring strategies for\\ntraining deep neural networks.Journal of Machine Learning Research, 10, 1–40. 533\\nLasserre, J.A., Bishop, C.M., andMinka, T.P.(2006). Principledhybridsofgenerativeand\\ndiscriminative models. InProceedings of the Computer Vision and Pattern Recognition\\nConference (CVPR’06), pages 87–94, Washington, DC, USA. IEEE Computer Society.\\n240, 250\\nLe, Q., Ngiam, J., Chen, Z., hao Chia, D. J., Koh, P. W., and Ng, A. (2010). Tiled\\nconvolutional neural networks. In J. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor,\\nR. Zemel, and A. Culotta, editors,Advances in Neural Information Processing Systems\\n23 (NIPS’10), pages 1279–1287. 346\\nLe, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., andNg, A.(2011). Onoptimization\\nmethods for deep learning. InProc. ICML’2011. ACM. 312\\nLe, Q., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., and Ng,\\nA. (2012). Building high-level features using large scale unsupervised learning. In\\nICML’2012. 22, 23\\nLe Roux, N. and Bengio, Y. (2008). Representational power of restricted Boltzmann\\nmachines and deep belief networks.Neural Computation, 20(6), 1631–1649. 551, 652\\nLe Roux, N. and Bengio, Y. (2010). Deep belief networks are compact universal approxi-\\nmators. Neural Computation, 22(8), 2192–2207. 551\\nLeCun, Y. (1985). Une procédure d’apprentissage pour Réseau à seuil assymétrique. In\\nCognitiva 85: A la Frontière de l’Intelligence Artiﬁcielle, des Sciences de la Connaissance\\net des Neurosciences, pages 599–604, Paris 1985. CESTA, Paris. 221\\n747'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 40}, page_content='BIBLIOGRAPHY\\nLeCun, Y. (1986). Learning processes in an asymmetric threshold network. In F. Fogelman-\\nSoulié, E. Bienenstock, and G. Weisbuch, editors,Disordered Systems and Biological\\nOrganization, pages 233–240. Springer-Verlag, Les Houches, France. 345\\nLeCun, Y. (1987).Modèles connexionistes de l’apprentissage. Ph.D. thesis, Université de\\nParis VI. 17, 499, 511\\nLeCun, Y. (1989). Generalization and network design strategies. Technical Report\\nCRG-TR-89-4, University of Toronto. 326, 345\\nLeCun, Y., Jackel, L. D., Boser, B., Denker, J. S., Graf, H. P., Guyon, I., Henderson, D.,\\nHoward, R. E., and Hubbard, W. (1989). Handwritten digit recognition: Applications\\nof neural network chips and automatic learning.IEEE Communications Magazine,\\n27(11), 41–46. 362\\nLeCun, Y., Bottou, L., Orr, G. B., and Müller, K.-R. (1998a). Eﬃcient backprop. In\\nNeural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524.\\nSpringer Verlag. 307, 424\\nLeCun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998b). Gradient based learning\\napplied to document recognition.Proc. IEEE. 15, 17, 19, 23, 365, 453, 455\\nLeCun, Y., Kavukcuoglu, K., and Farabet, C. (2010). Convolutional networks and\\napplications in vision. InCircuits and Systems (ISCAS), Proceedings of 2010 IEEE\\nInternational Symposium on, pages 253–256. IEEE. 365\\nL’Ecuyer, P. (1994). Eﬃciency improvement and variance reduction. InProceedings of\\nthe 1994 Winter Simulation Conference, pages 122––132. 687\\nLee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., and Tu, Z. (2014). Deeply-supervised nets.\\narXiv preprint arXiv:1409.5185. 322\\nLee, H., Battle, A., Raina, R., and Ng, A. (2007). Eﬃcient sparse coding algorithms.\\nIn B. Schölkopf, J. Platt, and T. Hoﬀman, editors,Advances in Neural Information\\nProcessing Systems 19 (NIPS’06), pages 801–808. MIT Press. 635\\nLee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net model for visual area\\nV2. In NIPS’07. 252\\nLee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009). Convolutional deep belief\\nnetworks for scalable unsupervised learning of hierarchical representations. In L. Bottou\\nand M. Littman, editors,Proceedings of the Twenty-sixth International Conference on\\nMachine Learning (ICML’09). ACM, Montreal, Canada. 357, 680, 681\\nLee, Y. J. and Grauman, K. (2011). Learning the easy things ﬁrst: self-paced visual\\ncategory discovery. InCVPR’2011. 324\\nLeibniz, G. W. (1676). Memoir using the chain rule. (Cited in TMME 7:2&3 p 321-332,\\n2010). 220\\n748'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 41}, page_content='BIBLIOGRAPHY\\nLenat, D. B. and Guha, R. V. (1989).Building large knowledge-based systems; representa-\\ntion and inference in the Cyc project. Addison-Wesley Longman Publishing Co., Inc.\\n2\\nLeshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. (1993). Multilayer feedforward\\nnetworks with a nonpolynomial activation function can approximate any function.\\nNeural Networks, 6, 861––867. 195, 196\\nLevenberg, K. (1944). A method for the solution of certain non-linear problems in least\\nsquares. Quarterly Journal of Applied Mathematics, II(2), 164–168. 308\\nL’Hôpital, G. F. A. (1696).Analyse des inﬁniment petits, pour l’intelligence des lignes\\ncourbes. Paris: L’Imprimerie Royale. 220\\nLi, Y., Swersky, K., and Zemel, R. S. (2015). Generative moment matching networks.\\nCoRR, abs/1502.02761. 699\\nLin, T., Horne, B. G., Tino, P., and Giles, C. L. (1996). Learning long-term dependencies\\nis not as diﬃcult with NARX recurrent neural networks.IEEE Transactions on Neural\\nNetworks, 7(6), 1329–1338. 402\\nLin, Y., Liu, Z., Sun, M., Liu, Y., and Zhu, X. (2015). Learning entity and relation\\nembeddings for knowledge graph completion. InProc. AAAI’15. 479\\nLinde, N. (1992). The machine that changed the world, episode 3. Documentary miniseries.\\n2\\nLindsey, C. and Lindblad, T. (1994). Review of hardware neural networks: a user’s\\nperspective. In Proc. Third Workshop on Neural Networks: From Biology to High\\nEnergy Physics, pages 195––202, Isola d’Elba, Italy. 446\\nLinnainmaa, S. (1976). Taylor expansion of the accumulated rounding error. BIT\\nNumerical Mathematics, 16(2), 146–160. 221\\nLong, P. M. and Servedio, R. A. (2010). Restricted Boltzmann machines are hard to\\napproximately evaluate or simulate. InProceedings of the 27th International Conference\\non Machine Learning (ICML’10). 655\\nLotter, W., Kreiman, G., and Cox, D. (2015). Unsupervised learning of visual structure\\nusing predictive generative networks.arXiv preprint arXiv:1511.06380. 542, 543\\nLovelace, A. (1842). Notes upon L. F. Menabrea’s “Sketch of the Analytical Engine\\ninvented by Charles Babbage”. 1\\nLu, L., Zhang, X., Cho, K., and Renals, S. (2015). A study of the recurrent neural network\\nencoder-decoder for large vocabulary speech recognition. InProc. Interspeech. 455\\nLu, T., Pál, D., and Pál, M. (2010). Contextual multi-armed bandits. InInternational\\nConference on Artiﬁcial Intelligence and Statistics, pages 485–492. 476\\n749'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 42}, page_content='BIBLIOGRAPHY\\nLuenberger, D. G. (1984).Linear and Nonlinear Programming. Addison Wesley. 312\\nLukoševičius, M. and Jaeger, H. (2009). Reservoir computing approaches to recurrent\\nneural network training.Computer Science Review, 3(3), 127–149. 399\\nLuo, H., Shen, R., Niu, C., and Ullrich, C. (2011). Learning class-relevant features and\\nclass-irrelevant features via a hybrid third-order RBM. InInternational Conference on\\nArtiﬁcial Intelligence and Statistics, pages 470–478. 683\\nLuo, H., Carrier, P. L., Courville, A., and Bengio, Y. (2013). Texture modeling with\\nconvolutional spike-and-slab RBMs and deep extensions. InAISTATS’2013. 100\\nLyu, S. (2009). Interpretation and generalization of score matching. InProceedings of the\\nTwenty-ﬁfth Conference in Uncertainty in Artiﬁcial Intelligence (UAI’09). 616\\nMa, J., Sheridan, R. P., Liaw, A., Dahl, G. E., and Svetnik, V. (2015). Deep neural nets\\nas a method for quantitative structure – activity relationships.J. Chemical information\\nand modeling. 528\\nMaas, A. L., Hannun, A. Y., and Ng, A. Y. (2013). Rectiﬁer nonlinearities improve neural\\nnetwork acoustic models. InICML Workshop on Deep Learning for Audio, Speech, and\\nLanguage Processing. 190\\nMaass, W. (1992). Bounds for the computational power and learning complexity of analog\\nneural nets (extended abstract). InProc. of the 25th ACM Symp. Theory of Computing,\\npages 335–344. 196\\nMaass, W., Schnitger, G., and Sontag, E. D. (1994). A comparison of the computational\\npower of sigmoid and Boolean threshold circuits. Theoretical Advances in Neural\\nComputation and Learning, pages 127–151. 196\\nMaass, W., Natschlaeger, T., and Markram, H. (2002). Real-time computing without\\nstable states: A new framework for neural computation based on perturbations.Neural\\nComputation, 14(11), 2531–2560. 399\\nMacKay, D. (2003).Information Theory, Inference and Learning Algorithms. Cambridge\\nUniversity Press. 71\\nMaclaurin, D., Duvenaud, D., and Adams, R. P. (2015). Gradient-based hyperparameter\\noptimization through reversible learning.arXiv preprint arXiv:1502.03492. 430\\nMao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., and Yuille, A. L. (2015). Deep captioning\\nwith multimodal recurrent neural networks. InICLR’2015. arXiv:1410.1090. 100\\nMarcotte, P. and Savard, G. (1992). Novel approaches to the discrimination problem.\\nZeitschrift für Operations Research (Theory), 36, 517–545. 273\\n750'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 43}, page_content='BIBLIOGRAPHY\\nMarlin, B. and de Freitas, N. (2011). Asymptotic eﬃciency of deterministic estimators for\\ndiscrete energy-based models: Ratio matching and pseudolikelihood. InUAI’2011. 615,\\n617\\nMarlin, B., Swersky, K., Chen, B., and de Freitas, N. (2010). Inductive principles for\\nrestricted Boltzmann machine learning. InProceedings of The Thirteenth International\\nConference on Artiﬁcial Intelligence and Statistics (AISTATS’10), volume 9, pages\\n509–516. 611, 616, 617\\nMarquardt, D. W. (1963). An algorithm for least-squares estimation of non-linear param-\\neters. Journal of the Society of Industrial and Applied Mathematics, 11(2), 431–441.\\n308\\nMarr, D. and Poggio, T. (1976). Cooperative computation of stereo disparity.Science,\\n194. 361\\nMartens, J. (2010). Deep learning via Hessian-free optimization. In L. Bottou and\\nM. Littman, editors,Proceedings of the Twenty-seventh International Conference on\\nMachine Learning (ICML-10), pages 735–742. ACM. 300\\nMartens, J. and Medabalimi, V. (2014). On the expressive eﬃciency of sum product\\nnetworks. arXiv:1411.7717. 551\\nMartens, J. and Sutskever, I. (2011). Learning recurrent neural networks with Hessian-free\\noptimization. In Proc. ICML’2011. ACM. 408\\nMase, S. (1995). Consistency of the maximum pseudo-likelihood estimator of continuous\\nstate space Gibbsian processes.The Annals of Applied Probability, 5(3), pp. 603–612.\\n614\\nMcClelland, J., Rumelhart, D., and Hinton, G. (1995). The appeal of parallel distributed\\nprocessing. In Computation & intelligence, pages 305–341. American Association for\\nArtiﬁcial Intelligence. 16\\nMcCulloch, W. S. and Pitts, W. (1943). A logical calculus of ideas immanent in nervous\\nactivity. Bulletin of Mathematical Biophysics, 5, 115–133. 13, 14\\nMead, C. and Ismail, M. (2012).Analog VLSI implementation of neural systems, volume 80.\\nSpringer Science & Business Media. 446\\nMelchior, J., Fischer, A., and Wiskott, L. (2013). How to center binary deep Boltzmann\\nmachines. arXiv preprint arXiv:1311.1354. 670\\nMemisevic, R. and Hinton, G. E. (2007). Unsupervised learning of image transformations.\\nIn Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR’07).\\n683\\n751'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 44}, page_content='BIBLIOGRAPHY\\nMemisevic, R. and Hinton, G. E. (2010). Learning to represent spatial transformations\\nwith factored higher-order Boltzmann machines.Neural Computation, 22(6), 1473–1492.\\n683\\nMesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E.,\\nMuller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra,\\nJ. (2011). Unsupervised and transfer learning challenge: a deep learning approach. In\\nJMLR W&CP: Proc. Unsupervised and Transfer Learning, volume 7. 197, 530, 536\\nMesnil, G., Rifai, S., Dauphin, Y., Bengio, Y., and Vincent, P. (2012). Surﬁng on the\\nmanifold. Learning Workshop, Snowbird. 707\\nMiikkulainen, R. and Dyer, M. G. (1991). Natural language processing with modular\\nPDP networks and distributed lexicon.Cognitive Science, 15, 343–399. 472\\nMikolov, T. (2012).Statistical Language Models based on Neural Networks. Ph.D. thesis,\\nBrno University of Technology. 410\\nMikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011a). Empirical\\nevaluation and combination of advanced language modeling techniques. InProc. 12th an-\\nnual conference of the international speech communication association (INTERSPEECH\\n2011). 467\\nMikolov, T., Deoras, A., Povey, D., Burget, L., and Cernocky, J. (2011b). Strategies for\\ntraining large scale neural network language models. InProc. ASRU’2011. 324, 467\\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Eﬃcient estimation of word rep-\\nresentations in vector space. InInternational Conference on Learning Representations:\\nWorkshops Track. 534\\nMikolov, T., Le, Q. V., and Sutskever, I. (2013b). Exploiting similarities among languages\\nfor machine translation. Technical report, arXiv:1309.4168. 537\\nMinka, T.(2005). Divergencemeasuresandmessagepassing. Microsoft Research Cambridge\\nUK Tech Rep MSRTR2005173, 72(TR-2005-173). 623\\nMinsky, M. L. and Papert, S. A. (1969).Perceptrons. MIT Press, Cambridge. 14\\nMirza, M. and Osindero, S. (2014). Conditional generative adversarial nets.arXiv preprint\\narXiv:1411.1784. 698\\nMishkin, D. and Matas, J. (2015). All you need is a good init. arXiv preprint\\narXiv:1511.06422. 301\\nMisra, J. and Saha, I. (2010). Artiﬁcial neural networks in hardware: A survey of two\\ndecades of progress.Neurocomputing, 74(1), 239–255. 446\\nMitchell, T. M. (1997).Machine Learning. McGraw-Hill, New York. 97\\n752'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 45}, page_content='BIBLIOGRAPHY\\nMiyato, T., Maeda, S., Koyama, M., Nakae, K., and Ishii, S. (2015). Distributional\\nsmoothing with virtual adversarial training. InICLR. Preprint: arXiv:1507.00677. 266\\nMnih, A. and Gregor, K. (2014). Neural variational inference and learning in belief\\nnetworks. InICML’2014. 688, 690\\nMnih, A. and Hinton, G. E. (2007). Three new graphical models for statistical language\\nmodelling. In Z. Ghahramani, editor,Proceedings of the Twenty-fourth International\\nConference on Machine Learning (ICML’07), pages 641–648. ACM. 460\\nMnih, A. and Hinton, G. E. (2009). A scalable hierarchical distributed language model.\\nIn D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors,Advances in Neural\\nInformation Processing Systems 21 (NIPS’08), pages 1081–1088. 462\\nMnih, A. and Kavukcuoglu, K. (2013). Learning word embeddings eﬃciently with noise-\\ncontrastive estimation. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and\\nK. Weinberger, editors,Advances in Neural Information Processing Systems 26, pages\\n2265–2273. Curran Associates, Inc. 467, 620\\nMnih, A. and Teh, Y. W. (2012). A fast and simple algorithm for training neural\\nprobabilistic language models. InICML’2012, pages 1751–1758. 467\\nMnih, V. and Hinton, G. (2010). Learning to detect roads in high-resolution aerial images.\\nIn Proceedings of the 11th European Conference on Computer Vision (ECCV). 100\\nMnih, V., Larochelle, H., and Hinton, G. (2011). Conditional restricted Boltzmann\\nmachines for structure output prediction. InProc. Conf. on Uncertainty in Artiﬁcial\\nIntelligence (UAI). 682\\nMnih, V., Kavukcuoglo, K., Silver, D., Graves, A., Antonoglou, I., and Wierstra, D. (2013).\\nPlaying Atari with deep reinforcement learning. Technical report, arXiv:1312.5602. 104\\nMnih, V., Heess, N., Graves, A., and Kavukcuoglu, K. (2014). Recurrent models of visual\\nattention. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger,\\neditors, NIPS’2014, pages 2204–2212. 688\\nMnih, V., Kavukcuoglo, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,\\nA., Riedmiller, M., Fidgeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,\\nAntonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015).\\nHuman-level control through deep reinforcement learning.Nature, 518, 529–533. 25\\nMobahi, H. and Fisher, III, J. W. (2015). A theoretical analysis of optimization by\\nGaussian continuation. InAAAI’2015. 323\\nMobahi, H., Collobert, R., and Weston, J. (2009). Deep learning from temporal coherence\\nin video. In L. Bottou and M. Littman, editors,Proceedings of the 26th International\\nConference on Machine Learning, pages 737–744, Montreal. Omnipress. 490\\n753'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 46}, page_content='BIBLIOGRAPHY\\nMohamed, A., Dahl, G., and Hinton, G. (2009). Deep belief networks for phone recognition.\\n454\\nMohamed, A., Sainath, T. N., Dahl, G., Ramabhadran, B., Hinton, G. E., and Picheny,\\nM.A.(2011). Deepbeliefnetworksusingdiscriminativefeaturesforphonerecognition. In\\nAcoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference\\non, pages 5060–5063. IEEE. 454\\nMohamed, A., Dahl, G., and Hinton, G. (2012a). Acoustic modeling using deep belief\\nnetworks. IEEE Trans. on Audio, Speech and Language Processing, 20(1), 14–22. 454\\nMohamed, A., Hinton, G., and Penn, G. (2012b). Understanding how deep belief networks\\nperform acoustic modelling. In Acoustics, Speech and Signal Processing (ICASSP),\\n2012 IEEE International Conference on, pages 4273–4276. IEEE. 454\\nMoller, M. F. (1993). A scaled conjugate gradient algorithm for fast supervised learning.\\nNeural Networks, 6, 525–533. 312\\nMontavon, G. and Muller, K.-R. (2012). Deep Boltzmann machines and the centering\\ntrick. In G. Montavon, G. Orr, and K.-R. Müller, editors,Neural Networks: Tricks of\\nthe Trade, volume 7700 ofLecture Notes in Computer Science, pages 621–637. Preprint:\\nhttp://arxiv.org/abs/1203.3783. 670\\nMontúfar, G. (2014). Universal approximation depth and errors of narrow belief networks\\nwith discrete units.Neural Computation, 26. 551\\nMontúfar, G. and Ay, N. (2011). Reﬁnements of universal approximation results for\\ndeep belief networks and restricted Boltzmann machines.Neural Computation, 23(5),\\n1306–1319. 551\\nMontufar, G. F., Pascanu, R., Cho, K., and Bengio, Y. (2014). On the number of linear\\nregions of deep neural networks. InNIPS’2014. 18, 196, 197\\nMor-Yosef, S., Samueloﬀ, A., Modan, B., Navot, D., and Schenker, J. G. (1990). Ranking\\nthe risk factors for cesarean: logistic regression analysis of a nationwide study.Obstet\\nGynecol, 75(6), 944–7. 3\\nMorin, F. and Bengio, Y. (2005). Hierarchical probabilistic neural network language\\nmodel. In AISTATS’2005. 462, 464\\nMozer, M. C. (1992). The induction of multiscale temporal structure. In J. M. S. Hanson\\nand R. Lippmann, editors, Advances in Neural Information Processing Systems 4\\n(NIPS’91), pages 275–282, San Mateo, CA. Morgan Kaufmann. 403\\nMurphy, K. P. (2012). Machine Learning: a Probabilistic Perspective. MIT Press,\\nCambridge, MA, USA. 60, 96, 142\\nMurray, B. U. I. and Larochelle, H. (2014). A deep and tractable density estimator. In\\nICML’2014. 186, 706, 707\\n754'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 47}, page_content='BIBLIOGRAPHY\\nNair, V. and Hinton, G. (2010). Rectiﬁed linear units improve restricted Boltzmann\\nmachines. InICML’2010. 15, 171, 193\\nNair, V. and Hinton, G. E. (2009). 3d object recognition with deep belief nets. In Y. Bengio,\\nD. Schuurmans, J. D. Laﬀerty, C. K. I. Williams, and A. Culotta, editors,Advances in\\nNeural Information Processing Systems 22, pages 1339–1347. Curran Associates, Inc.\\n683\\nNarayanan, H. and Mitter, S. (2010). Sample complexity of testing the manifold hypothesis.\\nIn NIPS’2010. 160\\nNaumann, U. (2008). Optimal Jacobian accumulation is NP-complete.Mathematical\\nProgramming, 112(2), 427–441. 218\\nNavigli, R. and Velardi, P. (2005). Structural semantic interconnections: a knowledge-\\nbased approach to word sense disambiguation.IEEE Trans. Pattern Analysis and\\nMachine Intelligence, 27(7), 1075––1086. 480\\nNeal, R. and Hinton, G. (1999). A view of the EM algorithm that justiﬁes incremental,\\nsparse, and other variants. In M. I. Jordan, editor,Learning in Graphical Models. MIT\\nPress, Cambridge, MA. 632\\nNeal, R. M. (1990). Learning stochastic feedforward networks. Technical report. 689\\nNeal, R. M. (1993). Probabilistic inference using Markov chain Monte-Carlo methods.\\nTechnical Report CRG-TR-93-1, Dept. of Computer Science, University of Toronto. 676\\nNeal, R. M. (1994). Sampling from multimodal distributions using tempered transitions.\\nTechnical Report 9421, Dept. of Statistics, University of Toronto. 601\\nNeal, R. M. (1996).Bayesian Learning for Neural Networks. Lecture Notes in Statistics.\\nSpringer. 262\\nNeal, R. M. (2001). Annealed importance sampling.Statistics and Computing, 11(2),\\n125–139. 623, 625, 626, 627\\nNeal, R. M. (2005). Estimating ratios of normalizing constants using linked importance\\nsampling. 627\\nNesterov, Y. (1983). A method of solving a convex programming problem with convergence\\nrate O(1/k2). Soviet Mathematics Doklady, 27, 372–376. 296\\nNesterov, Y. (2004).Introductory lectures on convex optimization : a basic course. Applied\\noptimization. Kluwer Academic Publ., Boston, Dordrecht, London. 296\\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading\\ndigits in natural images with unsupervised feature learning. Deep Learning and\\nUnsupervised Feature Learning Workshop, NIPS. 19\\n755'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 48}, page_content='BIBLIOGRAPHY\\nNey, H. and Kneser, R. (1993). Improved clustering techniques for class-based statistical\\nlanguage modelling. InEuropean Conference on Speech Communication and Technology\\n(Eurospeech), pages 973–976, Berlin. 458\\nNg, A. (2015). Advice for applying machine learning.\\nhttps://see.stanford.edu/materials/aimlcs229/ML-advice.pdf. 416\\nNiesler, T. R., Whittaker, E. W. D., and Woodland, P. C. (1998). Comparison of part-of-\\nspeech and automatically derived category-based language models for speech recognition.\\nIn International Conference on Acoustics, Speech and Signal Processing (ICASSP),\\npages 177–180. 458\\nNing, F., Delhomme, D., LeCun, Y., Piano, F., Bottou, L., and Barbano, P. E. (2005).\\nToward automatic phenotyping of developing embryos from videos.Image Processing,\\nIEEE Transactions on, 14(9), 1360–1371. 353\\nNocedal, J. and Wright, S. (2006).Numerical Optimization. Springer. 90, 93\\nNorouzi, M. and Fleet, D. J. (2011). Minimal loss hashing for compact binary codes. In\\nICML’2011. 523\\nNowlan, S. J. (1990). Competing experts: An experimental investigation of associative\\nmixture models. Technical Report CRG-TR-90-5, University of Toronto. 445\\nNowlan, S. J. and Hinton, G. E. (1992). Simplifying neural networks by soft weight-sharing.\\nNeural Computation, 4(4), 473–493. 137\\nOlshausen, B. and Field, D. J. (2005). How close are we to understanding V1?Neural\\nComputation, 17, 1665–1699. 15\\nOlshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive ﬁeld properties\\nby learning a sparse code for natural images.Nature, 381, 607–609. 144, 252, 364, 492\\nOlshausen, B. A., Anderson, C. H., and Van Essen, D. C. (1993). A neurobiological\\nmodel of visual attention and invariant pattern recognition based on dynamic routing\\nof information. J. Neurosci., 13(11), 4700–4719. 445\\nOpper, M. and Archambeau, C. (2009). The variational Gaussian approximation revisited.\\nNeural computation, 21(3), 786–792. 685\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. (2014). Learning and transferring mid-level\\nimage representations using convolutional neural networks. InComputer Vision and\\nPattern Recognition (CVPR), 2014 IEEE Conference on, pages 1717–1724. IEEE. 534\\nOsindero, S. and Hinton, G. E. (2008). Modeling image patches with a directed hierarchy\\nof Markov random ﬁelds. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors,\\nAdvances in Neural Information Processing Systems 20 (NIPS’07), pages 1121–1128,\\nCambridge, MA. MIT Press. 630\\n756'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 49}, page_content='BIBLIOGRAPHY\\nOvid and Martin, C. (2004).Metamorphoses. W.W. Norton. 1\\nPaccanaro, A. and Hinton, G. E. (2000). Extracting distributed representations of concepts\\nand relations from positive and negative propositions. InInternational Joint Conference\\non Neural Networks (IJCNN), Como, Italy. IEEE, New York. 479\\nPaine, T. L., Khorrami, P., Han, W., and Huang, T. S. (2014). An analysis of unsupervised\\npre-training in light of recent advances.arXiv preprint arXiv:1412.6597. 530\\nPalatucci, M., Pomerleau, D., Hinton, G. E., and Mitchell, T. M. (2009). Zero-shot\\nlearning with semantic output codes. In Y. Bengio, D. Schuurmans, J. D. Laﬀerty,\\nC. K. I. Williams, and A. Culotta, editors,Advances in Neural Information Processing\\nSystems 22, pages 1410–1418. Curran Associates, Inc. 537\\nParker, D. B. (1985). Learning-logic. Technical Report TR-47, Center for Comp. Research\\nin Economics and Management Sci., MIT. 221\\nPascanu, R., Mikolov, T., and Bengio, Y. (2013). On the diﬃculty of training recurrent\\nneural networks. InICML’2013. 285, 396, 399, 403, 409, 410, 411\\nPascanu, R., Gülçehre, Ç., Cho, K., and Bengio, Y. (2014a). How to construct deep\\nrecurrent neural networks. InICLR’2014. 18, 262, 393, 394, 406, 455\\nPascanu, R., Montufar, G., and Bengio, Y. (2014b). On the number of inference regions\\nof deep feed forward networks with piece-wise linear activations. InICLR’2014. 548\\nPati, Y., Rezaiifar, R., and Krishnaprasad, P. (1993). Orthogonal matching pursuit:\\nRecursive function approximation with applications to wavelet decomposition. InPro-\\nceedings of the 27 th Annual Asilomar Conference on Signals, Systems, and Computers,\\npages 40–44. 252\\nPearl, J. (1985). Bayesian networks: A model of self-activated memory for evidential\\nreasoning. In Proceedings of the 7th Conference of the Cognitive Science Society,\\nUniversity of California, Irvine, pages 329–334. 560\\nPearl, J. (1988).Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\\nInference. Morgan Kaufmann. 52\\nPerron, O. (1907). Zur theorie der matrices.Mathematische Annalen, 64(2), 248–263. 594\\nPetersen, K. B. and Pedersen, M. S. (2006). The matrix cookbook. Version 20051003. 29\\nPeterson, G. B. (2004). A day of great illumination: B. F. Skinner’s discovery of shaping.\\nJournal of the Experimental Analysis of Behavior, 82(3), 317–328. 324\\nPham, D.-T., Garat, P., and Jutten, C. (1992). Separation of a mixture of independent\\nsources through a maximum likelihood approach. InEUSIPCO, pages 771–774. 487\\n757'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 50}, page_content='BIBLIOGRAPHY\\nPham, P.-H., Jelaca, D., Farabet, C., Martini, B., LeCun, Y., and Culurciello, E. (2012).\\nNeuFlow: dataﬂow vision processing system-on-a-chip. InCircuits and Systems (MWS-\\nCAS), 2012 IEEE 55th International Midwest Symposium on, pages 1044–1047. IEEE.\\n446\\nPinheiro, P. H. O. and Collobert, R. (2014). Recurrent convolutional neural networks for\\nscene labeling. InICML’2014. 352, 353\\nPinheiro, P. H. O. and Collobert, R. (2015). From image-level to pixel-level labeling with\\nconvolutional networks. InConference on Computer Vision and Pattern Recognition\\n(CVPR). 352, 353\\nPinto, N., Cox, D. D., and DiCarlo, J. J. (2008). Why is real-world visual object recognition\\nhard? PLoS Comput Biol, 4. 451\\nPinto, N., Stone, Z., Zickler, T., and Cox, D. (2011). Scaling up biologically-inspired\\ncomputer vision: A case study in unconstrained face recognition on facebook. In\\nComputer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer\\nSociety Conference on, pages 35–42. IEEE. 357\\nPollack, J. B. (1990). Recursive distributed representations.Artiﬁcial Intelligence, 46(1),\\n77–105. 394\\nPolyak, B. and Juditsky, A. (1992). Acceleration of stochastic approximation by averaging.\\nSIAM J. Control and Optimization, 30(4), 838–855. 318\\nPolyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.\\nUSSR Computational Mathematics and Mathematical Physics, 4(5), 1–17. 292\\nPoole, B., Sohl-Dickstein, J., and Ganguli, S. (2014). Analyzing noise in autoencoders\\nand deep networks.CoRR, abs/1406.1831. 237\\nPoon, H. and Domingos, P. (2011). Sum-product networks: A new deep architecture. In\\nProceedings of the Twenty-seventh Conference in Uncertainty in Artiﬁcial Intelligence\\n(UAI), Barcelona, Spain. 551\\nPresley, R. K. and Haggard, R. L. (1994). A ﬁxed point implementation of the backpropa-\\ngation learning algorithm. InSoutheastcon’94. Creative Technology Transfer-A Global\\nAﬀair., Proceedings of the 1994 IEEE, pages 136–138. IEEE. 446\\nPrice, R. (1958). A useful theorem for nonlinear devices having Gaussian inputs.IEEE\\nTransactions on Information Theory, 4(2), 69–72. 685\\nQuiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., and Fried, I. (2005). Invariant visual\\nrepresentation by single neurons in the human brain.Nature, 435(7045), 1102–1107.\\n360\\n758'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 51}, page_content='BIBLIOGRAPHY\\nRadford, A., Metz, L., and Chintala, S. (2015). Unsupervised representation learning with\\ndeep convolutional generative adversarial networks.arXiv preprint arXiv:1511.06434.\\n549, 550, 698, 699\\nRaiko, T., Yao, L., Cho, K., and Bengio, Y. (2014). Iterative neural autoregressive\\ndistribution estimator (NADE-k). Technical report, arXiv:1406.1485. 671, 706\\nRaina, R., Madhavan, A., and Ng, A. Y. (2009). Large-scale deep unsupervised learning\\nusing graphics processors. In L. Bottou and M. Littman, editors,Proceedings of the\\nTwenty-sixth International Conference on Machine Learning (ICML’09), pages 873–880,\\nNew York, NY, USA. ACM. 23, 441\\nRamsey, F. P. (1926). Truth and probability. In R. B. Braithwaite, editor,The Foundations\\nof Mathematics and other Logical Essays, chapter 7, pages 156–198. McMaster University\\nArchive for the History of Economic Thought. 54\\nRanzato, M. and Hinton, G. H. (2010). Modeling pixel means and covariances using\\nfactorized third-order Boltzmann machines. InCVPR’2010, pages 2551–2558. 676\\nRanzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007a). Eﬃcient learning of sparse\\nrepresentations with an energy-based model. InNIPS’2006. 13, 18, 504, 526, 528\\nRanzato, M., Huang, F., Boureau, Y., and LeCun, Y. (2007b). Unsupervised learning of\\ninvariant feature hierarchies with applications to object recognition. InProceedings of\\nthe Computer Vision and Pattern Recognition Conference (CVPR’07). IEEE Press. 358\\nRanzato, M., Boureau, Y., and LeCun, Y. (2008). Sparse feature learning for deep belief\\nnetworks. InNIPS’2007. 504\\nRanzato, M., Krizhevsky, A., and Hinton, G. E. (2010a). Factored 3-way restricted\\nBoltzmann machines for modeling natural images. InProceedings of AISTATS 2010.\\n675\\nRanzato, M., Mnih, V., and Hinton, G. (2010b). Generating more realistic images using\\ngated MRFs. InNIPS’2010. 677\\nRao, C. (1945). Information and the accuracy attainable in the estimation of statistical\\nparameters. Bulletin of the Calcutta Mathematical Society, 37, 81–89. 133, 292\\nRasmus, A., Valpola, H., Honkala, M., Berglund, M., andRaiko, T.(2015). Semi-supervised\\nlearning with ladder network.arXiv preprint arXiv:1507.02672. 421, 528\\nRecht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild: A lock-free approach to\\nparallelizing stochastic gradient descent. InNIPS’2011. 442\\nReichert, D. P., Seriès, P., and Storkey, A. J. (2011). Neuronal adaptation for sampling-\\nbased probabilistic inference in perceptual bistability. InAdvances in Neural Information\\nProcessing Systems, pages 2357–2365. 663\\n759'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 52}, page_content='BIBLIOGRAPHY\\nRezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation\\nand approximate inference in deep generative models. In ICML’2014. Preprint:\\narXiv:1401.4082. 650, 685, 693\\nRifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a). Contractive auto-\\nencoders: Explicit invariance during feature extraction. InICML’2011. 518, 519, 520,\\n521\\nRifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., and Glorot, X.\\n(2011b). Higher order contractive auto-encoder. InECML PKDD. 518, 519\\nRifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011c). The manifold\\ntangent classiﬁer. InNIPS’2011. 268, 269, 520\\nRifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012). A generative process for\\nsampling contractive auto-encoders. InICML’2012. 707\\nRingach, D. and Shapley, R. (2004). Reverse correlation in neurophysiology.Cognitive\\nScience, 28(2), 147–166. 362\\nRoberts, S. and Everson, R. (2001).Independent component analysis: principles and\\npractice. Cambridge University Press. 489\\nRobinson, A. J. and Fallside, F. (1991). A recurrent error propagation network speech\\nrecognition system. Computer Speech and Language, 5(3), 259–274. 23, 454\\nRockafellar, R. T. (1997). Convex analysis. princeton landmarks in mathematics. 91\\nRomero, A., Ballas, N., Ebrahimi Kahou, S., Chassang, A., Gatta, C., and Bengio, Y.\\n(2015). Fitnets: Hints for thin deep nets. InICLR’2015, arXiv:1412.6550. 321\\nRosen, J. B. (1960). The gradient projection method for nonlinear programming. part i.\\nlinear constraints. Journal of the Society for Industrial and Applied Mathematics, 8(1),\\npp. 181–217. 91\\nRosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and\\norganization in the brain.Psychological Review, 65, 386–408. 13, 14, 23\\nRosenblatt, F. (1962).Principles of Neurodynamics. Spartan, New York. 14, 23\\nRoweis, S. and Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear\\nembedding. Science, 290(5500). 160, 516\\nRoweis, S., Saul, L., and Hinton, G. (2002). Global coordination of local linear models. In\\nT. Dietterich, S. Becker, and Z. Ghahramani, editors,Advances in Neural Information\\nProcessing Systems 14 (NIPS’01), Cambridge, MA. MIT Press. 485\\nRubin, D. B.et al.(1984). Bayesianly justiﬁable and relevant frequency calculations for\\nthe applied statistician.The Annals of Statistics, 12(4), 1151–1172. 712\\n760'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 53}, page_content='BIBLIOGRAPHY\\nRumelhart, D., Hinton, G., and Williams, R. (1986a). Learning representations by\\nback-propagating errors.Nature, 323, 533–536. 13, 17, 22, 200, 221, 367, 472, 477\\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986b). Learning internal represen-\\ntations by error propagation. In D. E. Rumelhart and J. L. McClelland, editors,Parallel\\nDistributed Processing, volume 1, chapter 8, pages 318–362. MIT Press, Cambridge. 19,\\n23, 221\\nRumelhart, D. E., McClelland, J. L., and the PDP Research Group (1986c).Parallel\\nDistributed Processing: Explorations in the Microstructure of Cognition. MIT Press,\\nCambridge. 16\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,\\nA., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2014a). ImageNet Large\\nScale Visual Recognition Challenge. 19\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,\\nA., Khosla, A., Bernstein, M.,et al.(2014b). Imagenet large scale visual recognition\\nchallenge. arXiv preprint arXiv:1409.0575. 24\\nRussel, S. J. and Norvig, P. (2003).Artiﬁcial Intelligence: a Modern Approach. Prentice\\nHall. 84\\nRust, N., Schwartz, O., Movshon, J. A., and Simoncelli, E. (2005). Spatiotemporal\\nelements of macaque V1 receptive ﬁelds.Neuron, 46(6), 945–956. 361\\nSainath, T., Mohamed, A., Kingsbury, B., and Ramabhadran, B. (2013). Deep convolu-\\ntional neural networks for LVCSR. InICASSP 2013. 455\\nSalakhutdinov, R. (2010). Learning in Markov random ﬁelds using tempered transitions. In\\nY. Bengio, D. Schuurmans, C. Williams, J. Laﬀerty, and A. Culotta, editors,Advances\\nin Neural Information Processing Systems 22 (NIPS’09). 601\\nSalakhutdinov, R. and Hinton, G. (2009a). Deep Boltzmann machines. InProceedings of\\nthe International Conference on Artiﬁcial Intelligence and Statistics, volume 5, pages\\n448–455. 22, 23, 527, 660, 663, 668, 669\\nSalakhutdinov, R. and Hinton, G. (2009b). Semantic hashing. InInternational Journal of\\nApproximate Reasoning. 522\\nSalakhutdinov, R. and Hinton, G. E. (2007a). Learning a nonlinear embedding by\\npreserving class neighbourhood structure. InProceedings of the Eleventh International\\nConference on Artiﬁcial Intelligence and Statistics (AISTATS’07), San Juan, Porto\\nRico. Omnipress. 525\\nSalakhutdinov, R. and Hinton, G. E. (2007b). Semantic hashing. InSIGIR’2007. 522\\n761'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 54}, page_content='BIBLIOGRAPHY\\nSalakhutdinov, R. and Hinton, G. E. (2008). Using deep belief nets to learn covariance\\nkernels for Gaussian processes. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors,\\nAdvances in Neural Information Processing Systems 20 (NIPS’07), pages 1249–1256,\\nCambridge, MA. MIT Press. 240\\nSalakhutdinov, R. and Larochelle, H. (2010). Eﬃcient learning of deep Boltzmann machines.\\nIn Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and\\nStatistics (AISTATS 2010), JMLR W&CP, volume 9, pages 693–700. 650\\nSalakhutdinov, R. and Mnih, A. (2008). Probabilistic matrix factorization. InNIPS’2008.\\n475\\nSalakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief\\nnetworks. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors,Proceedings of\\nthe Twenty-ﬁfth International Conference on Machine Learning (ICML’08), volume 25,\\npages 872–879. ACM. 626, 659\\nSalakhutdinov, R., Mnih, A., and Hinton, G. (2007). Restricted Boltzmann machines for\\ncollaborative ﬁltering. InICML. 475\\nSanger, T. D. (1994). Neural network learning control of robot manipulators using\\ngradually increasing task diﬃculty.IEEE Transactions on Robotics and Automation,\\n10(3). 324\\nSaul, L. K. and Jordan, M. I. (1996). Exploiting tractable substructures in intractable\\nnetworks. In D. Touretzky, M. Mozer, and M. Hasselmo, editors,Advances in Neural\\nInformation Processing Systems 8 (NIPS’95). MIT Press, Cambridge, MA. 636\\nSaul, L. K., Jaakkola, T., and Jordan, M. I. (1996). Mean ﬁeld theory for sigmoid belief\\nnetworks. Journal of Artiﬁcial Intelligence Research, 4, 61–76. 23, 689\\nSavich, A. W., Moussa, M., and Areibi, S. (2007). The impact of arithmetic representation\\non implementing mlp-bp on fpgas: A study.Neural Networks, IEEE Transactions on,\\n18(1), 240–252. 446\\nSaxe, A. M., Koh, P. W., Chen, Z., Bhand, M., Suresh, B., and Ng, A. (2011). On random\\nweights and unsupervised feature learning. InProc. ICML’2011. ACM. 357\\nSaxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear\\ndynamics of learning in deep linear neural networks. InICLR. 282, 283, 299\\nSchaul, T., Antonoglou, I., and Silver, D. (2014). Unit tests for stochastic optimization.\\nIn International Conference on Learning Representations. 306\\nSchmidhuber, J. (1992). Learning complex, extended sequences using the principle of\\nhistory compression. Neural Computation, 4(2), 234–242. 394\\nSchmidhuber, J. (1996). Sequential neural text compression.IEEE Transactions on Neural\\nNetworks, 7(1), 142–146. 472\\n762'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 55}, page_content='BIBLIOGRAPHY\\nSchmidhuber, J. (2012). Self-delimiting neural networks.arXiv preprint arXiv:1210.0118.\\n384\\nSchölkopf, B. and Smola, A. J. (2002).Learning with kernels: Support vector machines,\\nregularization, optimization, and beyond. MIT press. 700\\nSchölkopf, B., Smola, A., and Müller, K.-R. (1998). Nonlinear component analysis as a\\nkernel eigenvalue problem.Neural Computation, 10, 1299–1319. 160, 516\\nSchölkopf, B., Burges, C. J. C., and Smola, A. J. (1999).Advances in Kernel Methods —\\nSupport Vector Learning. MIT Press, Cambridge, MA. 17, 140\\nSchölkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., and Mooij, J. (2012). On\\ncausal and anticausal learning. InICML’2012, pages 1255–1262. 543\\nSchuster, M. (1999). On supervised learning from sequential data with applications for\\nspeech recognition. 186\\nSchuster, M. and Paliwal, K. (1997). Bidirectional recurrent neural networks.IEEE\\nTransactions on Signal Processing, 45(11), 2673–2681. 388\\nSchwenk, H. (2007). Continuous space language models.Computer speech and language,\\n21, 492–518. 461\\nSchwenk, H. (2010). Continuous space language models for statistical machine translation.\\nThe Prague Bulletin of Mathematical Linguistics, 93, 137–146. 468\\nSchwenk, H. (2014). Cleaned subset of WMT ’14 dataset. 19\\nSchwenk, H. and Bengio, Y. (1998). Training methods for adaptive boosting of neural net-\\nworks. In M. Jordan, M. Kearns, and S. Solla, editors,Advances in Neural Information\\nProcessing Systems 10 (NIPS’97), pages 647–653. MIT Press. 255\\nSchwenk, H. and Gauvain, J.-L. (2002). Connectionist language modeling for large\\nvocabulary continuous speech recognition. InInternational Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), pages 765–768, Orlando, Florida. 461\\nSchwenk, H., Costa-jussà, M. R., and Fonollosa, J. A. R. (2006). Continuous space\\nlanguage models for the IWSLT 2006 task. InInternational Workshop on Spoken\\nLanguage Translation, pages 166–173. 468\\nSeide, F., Li, G., and Yu, D. (2011). Conversational speech transcription using context-\\ndependent deep neural networks. InInterspeech 2011, pages 437–440. 24\\nSejnowski, T. (1987). Higher-order Boltzmann machines. InAIP Conference Proceedings\\n151 on Neural Networks for Computing, pages 398–403. American Institute of Physics\\nInc. 683\\n763'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 56}, page_content='BIBLIOGRAPHY\\nSeries, P., Reichert, D. P., and Storkey, A. J. (2010). Hallucinations in Charles Bonnet\\nsyndrome induced by homeostasis: a deep Boltzmann machine model. InAdvances in\\nNeural Information Processing Systems, pages 2020–2028. 663\\nSermanet, P., Chintala, S., and LeCun, Y. (2012). Convolutional neural networks applied\\nto house numbers digit classiﬁcation.CoRR, abs/1204.3968. 452\\nSermanet, P., Kavukcuoglu, K., Chintala, S., and LeCun, Y. (2013). Pedestrian detection\\nwith unsupervised multi-stage feature learning. InProc. International Conference on\\nComputer Vision and Pattern Recognition (CVPR’13). IEEE. 24, 197\\nShilov, G. (1977).Linear Algebra. Dover Books on Mathematics Series. Dover Publications.\\n29\\nSiegelmann, H. (1995). Computation beyond the Turing limit. Science, 268(5210),\\n545–548. 372\\nSiegelmann, H. and Sontag, E. (1991). Turing computability with neural nets.Applied\\nMathematics Letters, 4(6), 77–80. 372\\nSiegelmann, H. T. and Sontag, E. D. (1995). On the computational power of neural nets.\\nJournal of Computer and Systems Sciences, 50(1), 132–150. 372, 399\\nSietsma, J. and Dow, R. (1991). Creating artiﬁcial neural networks that generalize.Neural\\nNetworks, 4(1), 67–79. 237\\nSimard, D., Steinkraus, P. Y., and Platt, J. C. (2003). Best practices for convolutional\\nneural networks. InICDAR’2003. 365\\nSimard, P. and Graf, H. P. (1994). Backpropagation without multiplication. InAdvances\\nin Neural Information Processing Systems, pages 232–239. 446\\nSimard, P., Victorri, B., LeCun, Y., and Denker, J. (1992). Tangent prop - A formalism\\nfor specifying selected invariances in an adaptive network. InNIPS’1991. 267, 268, 269,\\n350\\nSimard, P. Y., LeCun, Y., and Denker, J. (1993). Eﬃcient pattern recognition using a\\nnew transformation distance. InNIPS’92. 267\\nSimard, P. Y., LeCun, Y. A., Denker, J. S., and Victorri, B. (1998). Transformation\\ninvariance in pattern recognition — tangent distance and tangent propagation.Lecture\\nNotes in Computer Science, 1524. 267\\nSimons, D. J. and Levin, D. T. (1998). Failure to detect changes to people during a\\nreal-world interaction.Psychonomic Bulletin & Review, 5(4), 644–649. 541\\nSimonyan, K. and Zisserman, A. (2015). Very deep convolutional networks for large-scale\\nimage recognition. InICLR. 319\\n764'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 57}, page_content='BIBLIOGRAPHY\\nSjöberg, J. and Ljung, L. (1995). Overtraining, regularization and searching for a minimum,\\nwith application to neural networks.International Journal of Control, 62(6), 1391–1407.\\n247\\nSkinner, B. F. (1958). Reinforcement today.American Psychologist, 13, 94–99. 324\\nSmolensky, P. (1986). Information processing in dynamical systems: Foundations of\\nharmony theory. In D. E. Rumelhart and J. L. McClelland, editors,Parallel Distributed\\nProcessing, volume 1, chapter 6, pages 194–281. MIT Press, Cambridge. 568, 584, 653\\nSnoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of\\nmachine learning algorithms. InNIPS’2012. 430\\nSocher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning, C. D. (2011a). Dynamic\\npooling and unfolding recursive autoencoders for paraphrase detection. InNIPS’2011.\\n394, 396\\nSocher, R., Manning, C., and Ng, A. Y. (2011b). Parsing natural scenes and natural lan-\\nguage with recursive neural networks. InProceedings of the Twenty-Eighth International\\nConference on Machine Learning (ICML’2011). 394\\nSocher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011c).\\nSemi-supervised recursive autoencoders for predicting sentiment distributions. In\\nEMNLP’2011. 394\\nSocher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., and Potts,\\nC. (2013a). Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In EMNLP’2013. 394, 396\\nSocher, R., Ganjoo, M., Manning, C. D., and Ng, A. Y. (2013b). Zero-shot learning through\\ncross-modal transfer. In 27th Annual Conference on Neural Information Processing\\nSystems (NIPS 2013). 537\\nSohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. (2015). Deep\\nunsupervised learning using nonequilibrium thermodynamics. 712\\nSohn, K., Zhou, G., and Lee, H. (2013). Learning and selecting features jointly with\\npoint-wise gated Boltzmann machines. InICML’2013. 683\\nSolomonoﬀ, R. J. (1989). A system for incremental learning based on algorithmic proba-\\nbility. 324\\nSontag, E. D. (1998). VC dimension of neural networks.NATO ASI Series F Computer\\nand Systems Sciences, 168, 69–96. 545, 549\\nSontag, E. D. and Sussman, H. J. (1989). Backpropagation can give rise to spurious local\\nminima even for networks without hidden layers.Complex Systems, 3, 91–106. 281\\n765'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 58}, page_content='BIBLIOGRAPHY\\nSparkes, B. (1996).The Red and the Black: Studies in Greek Pottery. Routledge. 1\\nSpitkovsky, V. I., Alshawi, H., and Jurafsky, D. (2010). From baby steps to leapfrog: how\\n“less is more” in unsupervised dependency parsing. InHLT’10. 324\\nSquire, W. and Trapp, G. (1998). Using complex variables to estimate derivatives of real\\nfunctions. SIAM Rev., 40(1), 110––112. 434\\nSrebro, N. and Shraibman, A. (2005). Rank, trace-norm and max-norm. InProceedings of\\nthe 18th Annual Conference on Learning Theory, pages 545–560. Springer-Verlag. 235\\nSrivastava, N. (2013). Improving Neural Networks With Dropout. Master’s thesis, U.\\nToronto. 533\\nSrivastava, N. and Salakhutdinov, R. (2012). Multimodal learning with deep Boltzmann\\nmachines. InNIPS’2012. 539\\nSrivastava, N., Salakhutdinov, R. R., and Hinton, G. E. (2013). Modeling documents with\\ndeep Boltzmann machines.arXiv preprint arXiv:1309.6865. 660\\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014).\\nDropout: A simple way to prevent neural networks from overﬁtting.Journal of Machine\\nLearning Research, 15, 1929–1958. 255, 261, 262, 263, 669\\nSrivastava, R. K., Greﬀ, K., and Schmidhuber, J. (2015). Highway networks.\\narXiv:1505.00387. 322\\nSteinkrau, D., Simard, P. Y., and Buck, I. (2005). Using GPUs for machine learning\\nalgorithms. 2013 12th International Conference on Document Analysis and Recognition,\\n0, 1115–1119. 440\\nStoyanov, V., Ropson, A., and Eisner, J. (2011). Empirical risk minimization of graphical\\nmodel parameters given approximate inference, decoding, and model structure. In\\nProceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics\\n(AISTATS), volume 15 ofJMLR Workshop and Conference Proceedings, pages 725–733,\\nFort Lauderdale. Supplementary material (4 pages) also available. 671, 695\\nSukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. (2015). Weakly supervised memory\\nnetworks. arXiv preprint arXiv:1503.08895. 412\\nSupancic, J. and Ramanan, D. (2013). Self-paced learning for long-term tracking. In\\nCVPR’2013. 324\\nSussillo, D. (2014). Random walks: Training very deep nonlinear feed-forward networks\\nwith smart initialization.CoRR, abs/1412.6558. 287, 300, 302, 398\\nSutskever, I. (2012).Training Recurrent Neural Networks. Ph.D. thesis, Department of\\ncomputer science, University of Toronto. 401, 408\\n766'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 59}, page_content='BIBLIOGRAPHY\\nSutskever, I. and Hinton, G. E. (2008). Deep narrow sigmoid belief networks are universal\\napproximators. Neural Computation, 20(11), 2629–2636. 689\\nSutskever, I. and Tieleman, T. (2010). On the Convergence Properties of Contrastive\\nDivergence. In Y. W. Teh and M. Titterington, editors,Proc. of the International\\nConference on Artiﬁcial Intelligence and Statistics (AISTATS), volume 9, pages 789–795.\\n610\\nSutskever, I., Hinton, G., and Taylor, G. (2009). The recurrent temporal restricted\\nBoltzmann machine. InNIPS’2008. 682\\nSutskever, I., Martens, J., and Hinton, G. E. (2011). Generating text with recurrent\\nneural networks. InICML’2011, pages 1017–1024. 472\\nSutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of\\ninitialization and momentum in deep learning. InICML. 296, 401, 408\\nSutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with\\nneural networks. InNIPS’2014, arXiv:1409.3215. 25, 99, 390, 404, 407, 469, 470\\nSutton, R. and Barto, A. (1998).Reinforcement Learning: An Introduction. MIT Press.\\n104\\nSutton, R. S., Mcallester, D., Singh, S., and Mansour, Y. (2000). Policy gradient methods\\nfor reinforcement learning with function approximation. InNIPS’1999, pages 1057–\\n–1063. MIT Press. 688\\nSwersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas, N. (2011). On\\nautoencoders and score matching for energy based models. InICML’2011. ACM. 509\\nSwersky, K., Snoek, J., and Adams, R. P. (2014). Freeze-thaw Bayesian optimization.\\narXiv preprint arXiv:1406.3896. 431\\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke,\\nV., and Rabinovich, A. (2014a). Going deeper with convolutions. Technical report,\\narXiv:1409.4842. 22, 23, 197, 255, 265, 322, 341\\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and\\nFergus, R. (2014b). Intriguing properties of neural networks.ICLR, abs/1312.6199.\\n265, 266, 269\\nSzegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., and Wojna, Z. (2015). Rethinking the\\nInception Architecture for Computer Vision.ArXiv e-prints. 240, 318\\nTaigman, Y., Yang, M., Ranzato, M., and Wolf, L. (2014). DeepFace: Closing the gap to\\nhuman-level performance in face veriﬁcation. InCVPR’2014. 98\\nTandy, D. W. (1997).Works and Days: A Translation and Commentary for the Social\\nSciences. University of California Press. 1\\n767'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 60}, page_content='BIBLIOGRAPHY\\nTang, Y. and Eliasmith, C. (2010). Deep networks for robust visual recognition. In\\nProceedings of the 27th International Conference on Machine Learning, June 21-24,\\n2010, Haifa, Israel. 237\\nTang, Y., Salakhutdinov, R., and Hinton, G. (2012). Deep mixtures of factor analysers.\\narXiv preprint arXiv:1206.4635. 485\\nTaylor, G. and Hinton, G. (2009). Factored conditional restricted Boltzmann machines\\nfor modeling motion style. In L. Bottou and M. Littman, editors,Proceedings of\\nthe Twenty-sixth International Conference on Machine Learning (ICML’09), pages\\n1025–1032, Montreal, Quebec, Canada. ACM. 682\\nTaylor, G., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary\\nlatent variables. In B. Schölkopf, J. Platt, and T. Hoﬀman, editors,Advances in Neural\\nInformation Processing Systems 19 (NIPS’06), pages 1345–1352. MIT Press, Cambridge,\\nMA. 682\\nTeh, Y., Welling, M., Osindero, S., and Hinton, G. E. (2003). Energy-based models\\nfor sparse overcomplete representations.Journal of Machine Learning Research, 4,\\n1235–1260. 487\\nTenenbaum, J., de Silva, V., and Langford, J. C. (2000). A global geometric framework\\nfor nonlinear dimensionality reduction.Science, 290(5500), 2319–2323. 160, 516, 532\\nTheis, L., van den Oord, A., and Bethge, M. (2015). A note on the evaluation of generative\\nmodels. arXiv:1511.01844. 694, 715\\nThompson, J., Jain, A., LeCun, Y., and Bregler, C. (2014). Joint training of a convolutional\\nnetwork and a graphical model for human pose estimation. InNIPS’2014. 353\\nThrun, S. (1995). Learning to play the game of chess. InNIPS’1994. 269\\nTibshirani, R. J. (1995). Regression shrinkage and selection via the lasso.Journal of the\\nRoyal Statistical Society B, 58, 267–288. 233\\nTieleman, T. (2008). Training restricted Boltzmann machines using approximations to\\nthe likelihood gradient. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors,Pro-\\nceedings of the Twenty-ﬁfth International Conference on Machine Learning (ICML’08),\\npages 1064–1071. ACM. 610\\nTieleman, T. and Hinton, G. (2009). Using fast weights to improve persistent contrastive\\ndivergence. In L. Bottou and M. Littman, editors,Proceedings of the Twenty-sixth\\nInternational Conference on Machine Learning (ICML’09), pages 1033–1040. ACM.\\n612\\nTipping, M. E. and Bishop, C. M. (1999). Probabilistic principal components analysis.\\nJournal of the Royal Statistical Society B, 61(3), 611–622. 487\\n768'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 61}, page_content='BIBLIOGRAPHY\\nTorralba, A., Fergus, R., and Weiss, Y. (2008). Small codes and large databases for\\nrecognition. InProceedings of the Computer Vision and Pattern Recognition Conference\\n(CVPR’08), pages 1–8. 522, 523\\nTouretzky, D. S. and Minton, G. E. (1985). Symbols among the neurons: Details of\\na connectionist inference architecture. InProceedings of the 9th International Joint\\nConference on Artiﬁcial Intelligence - Volume 1, IJCAI’85, pages238–243, San Francisco,\\nCA, USA. Morgan Kaufmann Publishers Inc. 16\\nTu, K. and Honavar, V. (2011). On the utility of curricula in unsupervised learning of\\nprobabilistic grammars. InIJCAI’2011. 324\\nTuraga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M., Briggman, K., Denk,\\nW., and Seung, H. S. (2010). Convolutional networks can learn to generate aﬃnity\\ngraphs for image segmentation.Neural Computation, 22(2), 511–538. 353\\nTurian, J., Ratinov, L., and Bengio, Y. (2010). Word representations: A simple and\\ngeneral method for semi-supervised learning. InProc. ACL’2010, pages 384–394. 533\\nTöscher, A., Jahrer, M., and Bell, R. M. (2009). The BigChaos solution to the Netﬂix\\ngrand prize. 475\\nUria, B., Murray, I., and Larochelle, H. (2013). Rnade: The real-valued neural autoregres-\\nsive density-estimator. InNIPS’2013. 706\\nvan den Oörd, A., Dieleman, S., and Schrauwen, B. (2013). Deep content-based music\\nrecommendation. In NIPS’2013. 475\\nvan der Maaten, L. and Hinton, G. E. (2008). Visualizing data using t-SNE.J. Machine\\nLearning Res., 9. 473, 516\\nVanhoucke, V., Senior, A., and Mao, M. Z. (2011). Improving the speed of neural networks\\non CPUs. InProc. Deep Learning and Unsupervised Feature Learning NIPS Workshop.\\n439, 447\\nVapnik, V. N. (1982).Estimation of Dependences Based on Empirical Data. Springer-\\nVerlag, Berlin. 112\\nVapnik, V. N. (1995).The Nature of Statistical Learning Theory. Springer, New York.\\n112\\nVapnik, V. N. and Chervonenkis, A. Y. (1971). On the uniform convergence of relative\\nfrequencies of events to their probabilities.Theory of Probability and Its Applications,\\n16, 264–280. 112\\nVincent, P. (2011). A connection between score matching and denoising autoencoders.\\nNeural Computation, 23(7). 509, 511, 708\\n769'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 62}, page_content='BIBLIOGRAPHY\\nVincent, P. and Bengio, Y. (2003). Manifold Parzen windows. InNIPS’2002. MIT Press.\\n517\\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and\\ncomposing robust features with denoising autoencoders. InICML 2008. 237, 511\\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked\\ndenoising autoencoders: Learning useful representations in a deep network with a local\\ndenoising criterion. J. Machine Learning Res., 11. 511\\nVincent, P., de Brébisson, A., and Bouthillier, X. (2015). Eﬃcient exact gradient update\\nfor training deep networks with very large sparse targets. In C. Cortes, N. D. Lawrence,\\nD. D. Lee, M. Sugiyama, and R. Garnett, editors,Advances in Neural Information\\nProcessing Systems 28, pages 1108–1116. Curran Associates, Inc. 460\\nVinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., and Hinton, G. (2014a).\\nGrammar as a foreign language. Technical report, arXiv:1412.7449. 404\\nVinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2014b). Show and tell: a neural image\\ncaption generator. arXiv 1411.4555. 404\\nVinyals, O., Fortunato, M., and Jaitly, N. (2015a). Pointer networks.arXiv preprint\\narXiv:1506.03134. 412\\nVinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015b). Show and tell: a neural image\\ncaption generator. InCVPR’2015. arXiv:1411.4555. 100\\nViola, P. and Jones, M. (2001). Robust real-time object detection. InInternational\\nJournal of Computer Vision. 444\\nVisin, F., Kastner, K., Cho, K., Matteucci, M., Courville, A., and Bengio, Y. (2015).\\nReNet: A recurrent neural network based alternative to convolutional networks.arXiv\\npreprint arXiv:1505.00393. 390\\nVon Melchner, L., Pallas, S. L., and Sur, M. (2000). Visual behaviour mediated by retinal\\nprojections directed to the auditory pathway.Nature, 404(6780), 871–876. 15\\nWager, S., Wang, S., and Liang, P. (2013). Dropout training as adaptive regularization.\\nIn Advances in Neural Information Processing Systems 26, pages 351–359. 262\\nWaibel, A., Hanazawa, T., Hinton, G. E., Shikano, K., and Lang, K. (1989). Phoneme\\nrecognition using time-delay neural networks.IEEE Transactions on Acoustics, Speech,\\nand Signal Processing, 37, 328–339. 368, 448, 454\\nWan, L., Zeiler, M., Zhang, S., LeCun, Y., and Fergus, R. (2013). Regularization of neural\\nnetworks using dropconnect. InICML’2013. 263\\nWang, S. and Manning, C. (2013). Fast dropout training. InICML’2013. 262\\n770'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 63}, page_content='BIBLIOGRAPHY\\nWang, Z., Zhang, J., Feng, J., and Chen, Z. (2014a). Knowledge graph and text jointly\\nembedding. In Proc. EMNLP’2014. 479\\nWang, Z., Zhang, J., Feng, J., and Chen, Z. (2014b). Knowledge graph embedding by\\ntranslating on hyperplanes. InProc. AAAI’2014. 479\\nWarde-Farley, D., Goodfellow, I. J., Courville, A., and Bengio, Y. (2014). An empirical\\nanalysis of dropout in piecewise linear networks. InICLR’2014. 259, 263, 264\\nWawrzynek, J., Asanovic, K., Kingsbury, B., Johnson, D., Beck, J., and Morgan, N.\\n(1996). Spert-II: A vector microprocessor system.Computer, 29(3), 79–86. 446\\nWeaver, L. and Tao, N. (2001). The optimal reward baseline for gradient-based reinforce-\\nment learning. InProc. UAI’2001, pages 538–545. 688\\nWeinberger, K. Q. and Saul, L. K. (2004). Unsupervised learning of image manifolds by\\nsemideﬁnite programming. InCVPR’2004, pages 988–995. 160, 516\\nWeiss, Y., Torralba, A., and Fergus, R. (2008). Spectral hashing. In NIPS, pages\\n1753–1760. 523\\nWelling, M., Zemel, R. S., and Hinton, G. E. (2002). Self supervised boosting. InAdvances\\nin Neural Information Processing Systems, pages 665–672. 699\\nWelling, M., Hinton, G. E., and Osindero, S. (2003a). Learning sparse topographic\\nrepresentations with products of Student-t distributions. InNIPS’2002. 677\\nWelling, M., Zemel, R., and Hinton, G. E. (2003b). Self-supervised boosting. In S. Becker,\\nS. Thrun, and K. Obermayer, editors,Advances in Neural Information Processing\\nSystems 15 (NIPS’02), pages 665–672. MIT Press. 621\\nWelling, M., Rosen-Zvi, M., and Hinton, G. E. (2005). Exponential family harmoniums\\nwith an application to information retrieval. In L. Saul, Y. Weiss, and L. Bottou,\\neditors, Advances in Neural Information Processing Systems 17 (NIPS’04), volume 17,\\nCambridge, MA. MIT Press. 673\\nWerbos, P. J. (1981). Applications of advances in nonlinear sensitivity analysis. In\\nProceedings of the 10th IFIP Conference, 31.8 - 4.9, NYC, pages 762–770. 221\\nWeston, J., Bengio, S., and Usunier, N. (2010). Large scale image annotation: learning to\\nrank with joint word-image embeddings.Machine Learning, 81(1), 21–35. 396\\nWeston, J., Chopra, S., and Bordes, A. (2014). Memory networks. arXiv preprint\\narXiv:1410.3916. 412, 480\\nWidrow, B. and Hoﬀ, M. E. (1960). Adaptive switching circuits. In1960 IRE WESCON\\nConvention Record, volume 4, pages 96–104. IRE, New York. 14, 19, 22, 23\\n771'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 64}, page_content='BIBLIOGRAPHY\\nWikipedia(2015). Listofanimalsbynumberofneurons—Wikipedia, thefreeencyclopedia.\\n[Online; accessed 4-March-2015]. 22, 23\\nWilliams, C. K. I. and Agakov, F. V. (2002). Products of Gaussians and Probabilistic\\nMinor Component Analysis.Neural Computation, 14(5), 1169–1182. 679\\nWilliams, C. K. I. and Rasmussen, C. E. (1996). Gaussian processes for regression. In\\nD. Touretzky, M. Mozer, and M. Hasselmo, editors,Advances in Neural Information\\nProcessing Systems 8 (NIPS’95), pages 514–520. MIT Press, Cambridge, MA. 140\\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms connectionist\\nreinforcement learning.Machine Learning, 8, 229–256. 685, 686\\nWilliams, R. J. and Zipser, D. (1989). A learning algorithm for continually running fully\\nrecurrent neural networks.Neural Computation, 1, 270–280. 219\\nWilson, D. R. and Martinez, T. R. (2003). The general ineﬃciency of batch training for\\ngradient descent learning.Neural Networks, 16(10), 1429–1451. 276\\nWilson, J. R. (1984). Variance reduction techniques for digital simulation.American\\nJournal of Mathematical and Management Sciences, 4(3), 277––312. 687\\nWiskott, L. and Sejnowski, T. J. (2002). Slow feature analysis: Unsupervised learning of\\ninvariances. Neural Computation, 14(4), 715–770. 489, 490\\nWolpert, D. and MacReady, W. (1997). No free lunch theorems for optimization.IEEE\\nTransactions on Evolutionary Computation, 1, 67–82. 289\\nWolpert, D. H. (1996). The lack of a priori distinction between learning algorithms.Neural\\nComputation, 8(7), 1341–1390. 114\\nWu, R., Yan, S., Shan, Y., Dang, Q., and Sun, G. (2015). Deep image: Scaling up image\\nrecognition. arXiv:1501.02876. 442\\nWu, Z. (1997). Global continuation for distance geometry problems.SIAM Journal of\\nOptimization, 7, 814–836. 323\\nXiong, H. Y., Barash, Y., and Frey, B. J. (2011). Bayesian prediction of tissue-regulated\\nsplicing using RNA sequence and cellular context.Bioinformatics, 27(18), 2554–2562.\\n262\\nXu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R. S., and\\nBengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual\\nattention. InICML’2015, arXiv:1502.03044. 100, 404, 688\\nYildiz, I. B., Jaeger, H., and Kiebel, S. J. (2012). Re-visiting the echo state property.\\nNeural networks, 35, 1–9. 400\\n772'), Document(metadata={'source': 'deep_learning_book.pdf', 'page': 65}, page_content='BIBLIOGRAPHY\\nYosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features\\nin deep neural networks? InNIPS’2014. 321, 534\\nYounes, L. (1998). On the convergence of Markovian stochastic algorithms with rapidly\\ndecreasing ergodicity rates. InStochastics and Stochastics Models, pages 177–228. 610\\nYu, D., Wang, S., and Deng, L. (2010). Sequential labeling using deep-structured\\nconditional random ﬁelds.IEEE Journal of Selected Topics in Signal Processing. 319\\nZaremba, W. and Sutskever, I. (2014). Learning to execute. arXiv 1410.4615. 325\\nZaremba, W. and Sutskever, I. (2015). Reinforcement learning neural Turing machines.\\narXiv:1505.00521. 415\\nZaslavsky, T. (1975).Facing Up to Arrangements: Face-Count Formulas for Partitions\\nof Space by Hyperplanes. Number no. 154 in Memoirs of the American Mathematical\\nSociety. American Mathematical Society. 548\\nZeiler, M. D. and Fergus, R. (2014). Visualizing and understanding convolutional networks.\\nIn ECCV’14. 6\\nZeiler, M. D., Ranzato, M., Monga, R., Mao, M., Yang, K., Le, Q., Nguyen, P., Senior,\\nA., Vanhoucke, V., Dean, J., and Hinton, G. E. (2013). On rectiﬁed linear units for\\nspeech processing. InICASSP 2013. 454\\nZhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. (2015). Object detectors\\nemerge in deep scene CNNs. ICLR’2015, arXiv:1412.6856. 549\\nZhou, J. and Troyanskaya, O. G. (2014). Deep supervised and convolutional generative\\nstochastic network for protein secondary structure prediction. InICML’2014. 711\\nZhou, Y. and Chellappa, R. (1988). Computation of optical ﬂow using a neural network.\\nIn Neural Networks, 1988., IEEE International Conference on, pages 71–78. IEEE. 335\\nZöhrer, M. and Pernkopf, F. (2014). General stochastic networks for classiﬁcation. In\\nNIPS’2014. 711\\n773')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "## file path\n",
    "file_path = \"deep_learning_book.pdf\"\n",
    "## create a pdf docs loader\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "## created a docs using pdfloader\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contents\\nWebsite viii\\nAcknowledgments ix\\nNotation xii\\n1 Introduction 1\\n1.1 Who Should Read This Book? . . . . . . . . . . . . . . . . . . . . 8\\n1.2 Historical Trends in Deep Learning . . . . . . . . . . . . . . . . . 12\\nI Applied Math and Machine Learning Basics 27\\n2 Linear Algebra 29\\n2.1 Scalars, Vectors, Matrices and Tensors . . . . . . . . . . . . . . . 29\\n2.2 Multiplying Matrices and Vectors . . . . . . . . . . . . . . . . . . 32\\n2.3 Identity and Inverse Matrices . . . . . . . . . . . . . . . . . . . . 34\\n2.4 Linear Dependence and Span . . . . . . . . . . . . . . . . . . . . 35\\n2.5 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.6 Special Kinds of Matrices and Vectors . . . . . . . . . . . . . . . 38\\n2.7 Eigendecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n2.8 Singular Value Decomposition . . . . . . . . . . . . . . . . . . . . 42\\n2.9 The Moore-Penrose Pseudoinverse . . . . . . . . . . . . . . . . . . 43\\n2.10 The Trace Operator . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n2.11 The Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n2.12 Example: Principal Components Analysis . . . . . . . . . . . . . 45\\n3 Probability and Information Theory 51\\n3.1 Why Probability? . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\ni'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
