{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9d71a1",
   "metadata": {},
   "source": [
    "### Load LLM model without any inbuild tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f4cd774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "load_dotenv()\n",
    "from groq import Groq\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a780b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325a97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "646760c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request/Response schema\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request/Response schema\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "\n",
    "\n",
    "class LLM_Model:\n",
    "    def __init__(self):\n",
    "        self.completion = \"Model loaded\"\n",
    "\n",
    "    def response_from_model(self, prompt: str = None) -> str:\n",
    "        # Simulate a response from the model\n",
    "        return \"I'm doing well, thank you!\"\n",
    "\n",
    "def chat(request: ChatRequest) -> ChatResponse:\n",
    "    user_input = request.message\n",
    "    llm = LLM_Model()\n",
    "    model_response = llm.response_from_model(user_input)\n",
    "    response = ChatResponse(response=model_response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd4f4b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: response=\"I'm doing well, thank you!\"\n"
     ]
    }
   ],
   "source": [
    "chat_response = chat(ChatRequest(message=\"Hello, how are you?\"))\n",
    "print(\"User Input:\", chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24cfa43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm doing well, thank you!\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LLM_Model()\n",
    "\n",
    "model.response_from_model(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "964a240e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatRequest(message='Hello, how are you?')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatRequest(message=\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bf71c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(response=\"I'm doing well, thank you!\")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatResponse(response=\"I'm doing well, thank you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca02e7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, how are you?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response = chat(ChatRequest(message=\"Hello, how are you?\"))\n",
    "chat_response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6ca78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1cd2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(request: ChatRequest):\n",
    "    try:\n",
    "        # Get response from GROQ LLM\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": request.message\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-8b-instant\",  # Using Llama 3 8B model\n",
    "            temperature=0.7,\n",
    "            max_tokens=1024,\n",
    "            top_p=1,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "        \n",
    "        response_text = chat_completion.choices[0].message.content\n",
    "        return ChatResponse(response=response_text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return ChatResponse(response=f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8c05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uvicorn\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Allows all origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  # Allows all methods\n",
    "    allow_headers=[\"*\"],  # Allows all headers\n",
    ")\n",
    "\n",
    "# Request/Response schema\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    try:\n",
    "        # Get response from GROQ LLM\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": request.message\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-8b-instant\",  # Using Llama 3 8B model\n",
    "            temperature=0.7,\n",
    "            max_tokens=1024,\n",
    "            top_p=1,\n",
    "            stream=False,\n",
    "            stop=None,\n",
    "        )\n",
    "        \n",
    "        response_text = chat_completion.choices[0].message.content\n",
    "        return ChatResponse(response=response_text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return ChatResponse(response=f\"Error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
